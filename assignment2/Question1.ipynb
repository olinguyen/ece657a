{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import average_precision_score, recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully loaded data!\n",
      "Data D features shape: (2200, 57)\n",
      "Labels shape: (2200,)\n"
     ]
    }
   ],
   "source": [
    "dataD = scipy.io.loadmat('./DataD.mat')\n",
    "\n",
    "features = dataD['fea']\n",
    "labels = dataD['gnd']\n",
    "labels[labels == -1] = 0\n",
    "labels = labels.ravel()\n",
    "\n",
    "print('Succesfully loaded data!')\n",
    "print(\"Data D features shape:\", features.shape)\n",
    "print(\"Labels shape:\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1100, 57)\n",
      "Test data shape: (1100, 57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype uint8 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "features_scaled = scale(features)\n",
    "half = int(len(features_scaled) / 2)\n",
    "\n",
    "X_train = features_scaled[:half]\n",
    "X_test = features_scaled[half:]\n",
    "\n",
    "y_train = labels[:half]\n",
    "y_test = labels[half:]\n",
    "\n",
    "print(\"Train data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. k-NN\n",
    "\n",
    "Using 5-fold cross validation\n",
    "(the crossvalind function can help) on the training set evaluate k-NN on\n",
    "the values k=[1, 3, 5, 7, ..., 31]. Plot a figure that shows the relationship between\n",
    "the accuracy and the parameter k. Report the best k in terms of classification accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(7, 0.71545454545454545),\n",
       "             (11, 0.71181818181818191),\n",
       "             (13, 0.71090909090909093),\n",
       "             (9, 0.70909090909090922),\n",
       "             (15, 0.70363636363636373),\n",
       "             (1, 0.70181818181818179),\n",
       "             (5, 0.70181818181818179),\n",
       "             (17, 0.69999999999999996),\n",
       "             (3, 0.69727272727272727),\n",
       "             (19, 0.68818181818181823),\n",
       "             (21, 0.68818181818181812),\n",
       "             (23, 0.68636363636363629),\n",
       "             (25, 0.67727272727272725),\n",
       "             (31, 0.67636363636363639),\n",
       "             (27, 0.67454545454545456),\n",
       "             (29, 0.67363636363636359)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "k_accuracies = {}\n",
    "\n",
    "k_neighbors = [i for i in range(1, 32, 2)]\n",
    "for k in k_neighbors:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    acc = []\n",
    "    for train, test in kf.split(X_train):\n",
    "        clf.fit(X_train[train], y_train[train])\n",
    "        acc.append(clf.score(X_train[test], y_train[test]))\n",
    "\n",
    "    accuracy = np.mean(acc)\n",
    "    k_accuracies[k] = accuracy\n",
    "    \n",
    "OrderedDict(sorted(k_accuracies.items(), key=itemgetter(1), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fd59fa78860>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAGHCAYAAABrpPKuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd59fb1b550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(k_accuracies.values())\n",
    "y = list(k_accuracies.keys())\n",
    "\n",
    "plt.plot(y, x)\n",
    "plt.title('k-NN Accuracy with different values of k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.516363636364 {'C': 2, 'gamma': 2.0, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "sigmas = [0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]\n",
    "gamma = [1/(2 * sigma**2) for sigma in sigmas]\n",
    "\n",
    "parameters = {'kernel':['rbf'], 'C':[0.1, 0.5, 1, 2, 5,10, 20, 50], 'gamma': gamma}\n",
    "svr = SVC()\n",
    "clf = GridSearchCV(svr, parameters, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best accuracy:\", clf.best_score_,  clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGHCAYAAACJeOnXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4yJOAtAjDHGGBNxFoAYY4wxJuL+B6xbTNacTPkBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd59fad17b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "svm = SVC(kernel='rbf', C=2, gamma=0.02)\n",
    "svm.fit(X_train, y_train)\n",
    "y_score = svm.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve for SVM with best parameters')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying using the test set with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN test accuracy: 0.717272727273\n",
      "SVM test accuracy 0.902727272727\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn_acc = knn.score(X_test, y_test)\n",
    "\n",
    "svm = SVC(kernel='rbf', C=2, gamma=0.02)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_acc = svm.score(X_test, y_test)\n",
    "\n",
    "print(\"KNN test accuracy:\", knn_acc)\n",
    "print(\"SVM test accuracy\", svm_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree test accuracy: 0.904545454545\n"
     ]
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=1, max_leaf_nodes=(len(X_train) - 1))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "dt_acc = clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Decision tree test accuracy:\", dt_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree test accuracy: 0.936363636364\n"
     ]
    }
   ],
   "source": [
    "randomforest = RandomForestClassifier(min_samples_split=10, min_samples_leaf=1, max_leaf_nodes=(len(X_train) - 1))\n",
    "\n",
    "randomforest.fit(X_train, y_train)\n",
    "dt_acc = randomforest.score(X_test, y_test)\n",
    "print(\"Random forest test accuracy:\", dt_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 2) (1100, 2)\n",
      "1056/1100 [===========================>..] - ETA: 0s\n",
      "Neural network accuracy: 0.902727272727\n",
      "7.945659637451172\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels\n",
    "y_train_onehot = np_utils.to_categorical(y_train)\n",
    "y_test_onehot = np_utils.to_categorical(y_test)\n",
    "    \n",
    "num_features = features.shape[1]\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "print(y_train_onehot.shape, y_test_onehot.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(num_features, input_dim=num_features, init='normal', activation='relu'))\n",
    "model.add(Dense(num_classes, init='normal', activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "ts = time.time()\n",
    "model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), nb_epoch=100, batch_size=250, verbose=0)\n",
    "te = time.time()\n",
    "\n",
    "loss, nn_acc = model.evaluate(X_test, y_test_onehot)\n",
    "\n",
    "print(\"\\nNeural network accuracy:\", nn_acc)\n",
    "print(te - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosted tree test accuracy: 0.971818181818\n"
     ]
    }
   ],
   "source": [
    "\n",
    "randomforest = GradientBoostingClassifier()\n",
    "\n",
    "randomforest.fit(X_train, y_train)\n",
    "dt_acc = randomforest.score(X_test, y_test)\n",
    "print(\"Gradient boosted tree test accuracy:\", dt_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1100 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 1s - loss: 7.8497 - acc: 0.4800 - val_loss: 7.6038 - val_acc: 0.4864\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s - loss: 7.7407 - acc: 0.4800 - val_loss: 7.4493 - val_acc: 0.4864\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s - loss: 7.6089 - acc: 0.4800 - val_loss: 7.2598 - val_acc: 0.4864\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s - loss: 7.4064 - acc: 0.4800 - val_loss: 7.0150 - val_acc: 0.4864\n",
      "Epoch 5/100\n",
      "1100/1100 [==============================] - 0s - loss: 7.1364 - acc: 0.4809 - val_loss: 6.7112 - val_acc: 0.4864\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s - loss: 6.8187 - acc: 0.4809 - val_loss: 6.3337 - val_acc: 0.4864\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s - loss: 6.4112 - acc: 0.4818 - val_loss: 5.8783 - val_acc: 0.4873\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s - loss: 5.9140 - acc: 0.4818 - val_loss: 5.3446 - val_acc: 0.4873\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s - loss: 5.3570 - acc: 0.4809 - val_loss: 4.7419 - val_acc: 0.4882\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s - loss: 4.7401 - acc: 0.4782 - val_loss: 4.0985 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s - loss: 4.0910 - acc: 0.4773 - val_loss: 3.4582 - val_acc: 0.4891\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s - loss: 3.4465 - acc: 0.4755 - val_loss: 2.8337 - val_acc: 0.4936\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s - loss: 2.8319 - acc: 0.4718 - val_loss: 2.2607 - val_acc: 0.4945\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s - loss: 2.2679 - acc: 0.4773 - val_loss: 1.7834 - val_acc: 0.5027\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.8237 - acc: 0.4782 - val_loss: 1.4534 - val_acc: 0.5027\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.5309 - acc: 0.4791 - val_loss: 1.2958 - val_acc: 0.5009\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.4098 - acc: 0.4845 - val_loss: 1.2480 - val_acc: 0.5000\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.3567 - acc: 0.4945 - val_loss: 1.2177 - val_acc: 0.4973\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.3109 - acc: 0.4982 - val_loss: 1.1715 - val_acc: 0.4945\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.2513 - acc: 0.4945 - val_loss: 1.1158 - val_acc: 0.4945\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.1914 - acc: 0.4955 - val_loss: 1.0604 - val_acc: 0.4955\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.1336 - acc: 0.4927 - val_loss: 1.0135 - val_acc: 0.4945\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.0818 - acc: 0.4973 - val_loss: 0.9716 - val_acc: 0.5018\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s - loss: 1.0390 - acc: 0.5009 - val_loss: 0.9358 - val_acc: 0.4973\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.9999 - acc: 0.5009 - val_loss: 0.9057 - val_acc: 0.4927\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.9672 - acc: 0.5064 - val_loss: 0.8789 - val_acc: 0.5009\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.9373 - acc: 0.5009 - val_loss: 0.8560 - val_acc: 0.5036\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.9106 - acc: 0.4964 - val_loss: 0.8360 - val_acc: 0.5100\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8876 - acc: 0.5018 - val_loss: 0.8186 - val_acc: 0.5109\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8667 - acc: 0.5064 - val_loss: 0.8036 - val_acc: 0.5027\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8478 - acc: 0.5100 - val_loss: 0.7911 - val_acc: 0.4982\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8330 - acc: 0.5100 - val_loss: 0.7805 - val_acc: 0.4973\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8203 - acc: 0.5073 - val_loss: 0.7715 - val_acc: 0.5055\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8099 - acc: 0.5118 - val_loss: 0.7636 - val_acc: 0.5009\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.8002 - acc: 0.5136 - val_loss: 0.7570 - val_acc: 0.5082\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7928 - acc: 0.5100 - val_loss: 0.7508 - val_acc: 0.5136\n",
      "Epoch 37/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7857 - acc: 0.5109 - val_loss: 0.7454 - val_acc: 0.5200\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7799 - acc: 0.5109 - val_loss: 0.7403 - val_acc: 0.5236\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7745 - acc: 0.5064 - val_loss: 0.7355 - val_acc: 0.5236\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7698 - acc: 0.5036 - val_loss: 0.7314 - val_acc: 0.5255\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7655 - acc: 0.5000 - val_loss: 0.7280 - val_acc: 0.5245\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7616 - acc: 0.5045 - val_loss: 0.7251 - val_acc: 0.5200\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7579 - acc: 0.5045 - val_loss: 0.7226 - val_acc: 0.5227\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7549 - acc: 0.5036 - val_loss: 0.7203 - val_acc: 0.5191\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7517 - acc: 0.5045 - val_loss: 0.7184 - val_acc: 0.5218\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7490 - acc: 0.5073 - val_loss: 0.7166 - val_acc: 0.5236\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7462 - acc: 0.5091 - val_loss: 0.7151 - val_acc: 0.5282\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7436 - acc: 0.5055 - val_loss: 0.7138 - val_acc: 0.5300\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7412 - acc: 0.5055 - val_loss: 0.7126 - val_acc: 0.5336\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7391 - acc: 0.5045 - val_loss: 0.7116 - val_acc: 0.5318\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7371 - acc: 0.5018 - val_loss: 0.7107 - val_acc: 0.5309\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7349 - acc: 0.5036 - val_loss: 0.7099 - val_acc: 0.5291\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7333 - acc: 0.5055 - val_loss: 0.7092 - val_acc: 0.5291\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7314 - acc: 0.5091 - val_loss: 0.7085 - val_acc: 0.5291\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7300 - acc: 0.5082 - val_loss: 0.7079 - val_acc: 0.5309\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7283 - acc: 0.5118 - val_loss: 0.7074 - val_acc: 0.5282\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7269 - acc: 0.5118 - val_loss: 0.7069 - val_acc: 0.5291\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7255 - acc: 0.5127 - val_loss: 0.7065 - val_acc: 0.5264\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7241 - acc: 0.5127 - val_loss: 0.7061 - val_acc: 0.5200\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7228 - acc: 0.5118 - val_loss: 0.7058 - val_acc: 0.5173\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7215 - acc: 0.5118 - val_loss: 0.7055 - val_acc: 0.5155\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7205 - acc: 0.5136 - val_loss: 0.7052 - val_acc: 0.5127\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7194 - acc: 0.5136 - val_loss: 0.7050 - val_acc: 0.5127\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7185 - acc: 0.5145 - val_loss: 0.7048 - val_acc: 0.5145\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7176 - acc: 0.5173 - val_loss: 0.7046 - val_acc: 0.5127\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7168 - acc: 0.5173 - val_loss: 0.7045 - val_acc: 0.5109\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7160 - acc: 0.5155 - val_loss: 0.7043 - val_acc: 0.5118\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7152 - acc: 0.5109 - val_loss: 0.7041 - val_acc: 0.5155\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7144 - acc: 0.5127 - val_loss: 0.7040 - val_acc: 0.5164\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7137 - acc: 0.5118 - val_loss: 0.7039 - val_acc: 0.5155\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7128 - acc: 0.5127 - val_loss: 0.7038 - val_acc: 0.5155\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7121 - acc: 0.5127 - val_loss: 0.7036 - val_acc: 0.5164\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7114 - acc: 0.5109 - val_loss: 0.7035 - val_acc: 0.5164\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7107 - acc: 0.5109 - val_loss: 0.7035 - val_acc: 0.5173\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7100 - acc: 0.5127 - val_loss: 0.7034 - val_acc: 0.5173\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7095 - acc: 0.5127 - val_loss: 0.7034 - val_acc: 0.5173\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7088 - acc: 0.5145 - val_loss: 0.7034 - val_acc: 0.5191\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7082 - acc: 0.5191 - val_loss: 0.7034 - val_acc: 0.5200\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7078 - acc: 0.5182 - val_loss: 0.7034 - val_acc: 0.5182\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7074 - acc: 0.5191 - val_loss: 0.7034 - val_acc: 0.5191\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7070 - acc: 0.5200 - val_loss: 0.7032 - val_acc: 0.5164\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7066 - acc: 0.5209 - val_loss: 0.7030 - val_acc: 0.5155\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7062 - acc: 0.5209 - val_loss: 0.7029 - val_acc: 0.5164\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7058 - acc: 0.5209 - val_loss: 0.7027 - val_acc: 0.5173\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7055 - acc: 0.5218 - val_loss: 0.7026 - val_acc: 0.5173\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7052 - acc: 0.5209 - val_loss: 0.7025 - val_acc: 0.5182\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7049 - acc: 0.5218 - val_loss: 0.7024 - val_acc: 0.5173\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7046 - acc: 0.5218 - val_loss: 0.7023 - val_acc: 0.5173\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7043 - acc: 0.5227 - val_loss: 0.7022 - val_acc: 0.5164\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7040 - acc: 0.5227 - val_loss: 0.7021 - val_acc: 0.5164\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7037 - acc: 0.5236 - val_loss: 0.7020 - val_acc: 0.5145\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7034 - acc: 0.5264 - val_loss: 0.7019 - val_acc: 0.5136\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7032 - acc: 0.5282 - val_loss: 0.7019 - val_acc: 0.5136\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7029 - acc: 0.5300 - val_loss: 0.7017 - val_acc: 0.5136\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7027 - acc: 0.5300 - val_loss: 0.7015 - val_acc: 0.5136\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7024 - acc: 0.5291 - val_loss: 0.7014 - val_acc: 0.5127\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7021 - acc: 0.5282 - val_loss: 0.7013 - val_acc: 0.5100\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7019 - acc: 0.5282 - val_loss: 0.7011 - val_acc: 0.5127\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7016 - acc: 0.5264 - val_loss: 0.7010 - val_acc: 0.5127\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7013 - acc: 0.5245 - val_loss: 0.7009 - val_acc: 0.5109\n",
      "Train on 1100 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7010 - acc: 0.5245 - val_loss: 0.7008 - val_acc: 0.5100\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7008 - acc: 0.5264 - val_loss: 0.7007 - val_acc: 0.5109\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7005 - acc: 0.5245 - val_loss: 0.7006 - val_acc: 0.5109\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7003 - acc: 0.5236 - val_loss: 0.7004 - val_acc: 0.5100\n",
      "Epoch 5/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7001 - acc: 0.5227 - val_loss: 0.7003 - val_acc: 0.5109\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.7000 - acc: 0.5245 - val_loss: 0.7002 - val_acc: 0.5109\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6997 - acc: 0.5245 - val_loss: 0.7002 - val_acc: 0.5109\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6996 - acc: 0.5255 - val_loss: 0.7002 - val_acc: 0.5100\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6994 - acc: 0.5255 - val_loss: 0.7001 - val_acc: 0.5109\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6992 - acc: 0.5245 - val_loss: 0.7001 - val_acc: 0.5118\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6990 - acc: 0.5255 - val_loss: 0.7000 - val_acc: 0.5109\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6989 - acc: 0.5245 - val_loss: 0.7000 - val_acc: 0.5109\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6987 - acc: 0.5255 - val_loss: 0.7000 - val_acc: 0.5100\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6986 - acc: 0.5255 - val_loss: 0.7000 - val_acc: 0.5091\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6984 - acc: 0.5255 - val_loss: 0.7000 - val_acc: 0.5073\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6983 - acc: 0.5255 - val_loss: 0.7000 - val_acc: 0.5082\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6982 - acc: 0.5255 - val_loss: 0.6999 - val_acc: 0.5064\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6981 - acc: 0.5264 - val_loss: 0.6997 - val_acc: 0.5055\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6980 - acc: 0.5255 - val_loss: 0.6997 - val_acc: 0.5055\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6978 - acc: 0.5264 - val_loss: 0.6997 - val_acc: 0.5055\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6977 - acc: 0.5282 - val_loss: 0.6998 - val_acc: 0.5055\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6977 - acc: 0.5282 - val_loss: 0.6997 - val_acc: 0.5055\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6975 - acc: 0.5282 - val_loss: 0.6996 - val_acc: 0.5055\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6974 - acc: 0.5282 - val_loss: 0.6995 - val_acc: 0.5064\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6973 - acc: 0.5273 - val_loss: 0.6993 - val_acc: 0.5055\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6972 - acc: 0.5273 - val_loss: 0.6992 - val_acc: 0.5064\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6971 - acc: 0.5273 - val_loss: 0.6991 - val_acc: 0.5064\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6970 - acc: 0.5273 - val_loss: 0.6991 - val_acc: 0.5064\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6968 - acc: 0.5282 - val_loss: 0.6993 - val_acc: 0.5073\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6967 - acc: 0.5282 - val_loss: 0.6993 - val_acc: 0.5082\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6967 - acc: 0.5273 - val_loss: 0.6993 - val_acc: 0.5082\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6965 - acc: 0.5273 - val_loss: 0.6991 - val_acc: 0.5082\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6964 - acc: 0.5282 - val_loss: 0.6991 - val_acc: 0.5073\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6963 - acc: 0.5282 - val_loss: 0.6990 - val_acc: 0.5082\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6962 - acc: 0.5282 - val_loss: 0.6989 - val_acc: 0.5091\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6961 - acc: 0.5291 - val_loss: 0.6989 - val_acc: 0.5082\n",
      "Epoch 37/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6960 - acc: 0.5282 - val_loss: 0.6989 - val_acc: 0.5100\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6959 - acc: 0.5282 - val_loss: 0.6990 - val_acc: 0.5100\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6957 - acc: 0.5255 - val_loss: 0.6990 - val_acc: 0.5100\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6957 - acc: 0.5264 - val_loss: 0.6990 - val_acc: 0.5091\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6956 - acc: 0.5273 - val_loss: 0.6989 - val_acc: 0.5100\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6954 - acc: 0.5291 - val_loss: 0.6988 - val_acc: 0.5100\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6953 - acc: 0.5282 - val_loss: 0.6989 - val_acc: 0.5091\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6952 - acc: 0.5273 - val_loss: 0.6988 - val_acc: 0.5091\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6951 - acc: 0.5282 - val_loss: 0.6988 - val_acc: 0.5091\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6950 - acc: 0.5282 - val_loss: 0.6987 - val_acc: 0.5091\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6949 - acc: 0.5264 - val_loss: 0.6986 - val_acc: 0.5091\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6948 - acc: 0.5264 - val_loss: 0.6987 - val_acc: 0.5082\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6947 - acc: 0.5273 - val_loss: 0.6985 - val_acc: 0.5082\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6946 - acc: 0.5273 - val_loss: 0.6984 - val_acc: 0.5091\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6946 - acc: 0.5282 - val_loss: 0.6982 - val_acc: 0.5100\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6945 - acc: 0.5282 - val_loss: 0.6982 - val_acc: 0.5091\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6944 - acc: 0.5282 - val_loss: 0.6983 - val_acc: 0.5091\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6943 - acc: 0.5273 - val_loss: 0.6983 - val_acc: 0.5082\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6942 - acc: 0.5273 - val_loss: 0.6984 - val_acc: 0.5082\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6942 - acc: 0.5273 - val_loss: 0.6984 - val_acc: 0.5073\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6941 - acc: 0.5273 - val_loss: 0.6983 - val_acc: 0.5073\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6941 - acc: 0.5273 - val_loss: 0.6979 - val_acc: 0.5091\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6940 - acc: 0.5291 - val_loss: 0.6978 - val_acc: 0.5091\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6939 - acc: 0.5309 - val_loss: 0.6978 - val_acc: 0.5091\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6938 - acc: 0.5291 - val_loss: 0.6980 - val_acc: 0.5091\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6937 - acc: 0.5300 - val_loss: 0.6981 - val_acc: 0.5073\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6936 - acc: 0.5300 - val_loss: 0.6981 - val_acc: 0.5073\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6936 - acc: 0.5309 - val_loss: 0.6979 - val_acc: 0.5100\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6935 - acc: 0.5300 - val_loss: 0.6977 - val_acc: 0.5091\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6934 - acc: 0.5309 - val_loss: 0.6978 - val_acc: 0.5091\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6933 - acc: 0.5300 - val_loss: 0.6979 - val_acc: 0.5100\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6933 - acc: 0.5291 - val_loss: 0.6980 - val_acc: 0.5100\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6933 - acc: 0.5300 - val_loss: 0.6978 - val_acc: 0.5109\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6932 - acc: 0.5309 - val_loss: 0.6976 - val_acc: 0.5109\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6932 - acc: 0.5318 - val_loss: 0.6975 - val_acc: 0.5109\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6932 - acc: 0.5327 - val_loss: 0.6974 - val_acc: 0.5100\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6931 - acc: 0.5327 - val_loss: 0.6976 - val_acc: 0.5091\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6930 - acc: 0.5318 - val_loss: 0.6977 - val_acc: 0.5100\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6930 - acc: 0.5309 - val_loss: 0.6976 - val_acc: 0.5100\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6929 - acc: 0.5327 - val_loss: 0.6974 - val_acc: 0.5082\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6929 - acc: 0.5327 - val_loss: 0.6974 - val_acc: 0.5073\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6928 - acc: 0.5345 - val_loss: 0.6976 - val_acc: 0.5082\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6928 - acc: 0.5318 - val_loss: 0.6977 - val_acc: 0.5073\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6928 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5082\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6927 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5082\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6926 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5082\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6926 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5073\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6926 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5073\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6925 - acc: 0.5327 - val_loss: 0.6975 - val_acc: 0.5082\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6925 - acc: 0.5318 - val_loss: 0.6974 - val_acc: 0.5091\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6924 - acc: 0.5318 - val_loss: 0.6975 - val_acc: 0.5091\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6924 - acc: 0.5327 - val_loss: 0.6976 - val_acc: 0.5082\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6924 - acc: 0.5318 - val_loss: 0.6975 - val_acc: 0.5073\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6923 - acc: 0.5327 - val_loss: 0.6972 - val_acc: 0.5073\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6923 - acc: 0.5355 - val_loss: 0.6972 - val_acc: 0.5073\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6923 - acc: 0.5355 - val_loss: 0.6973 - val_acc: 0.5073\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6922 - acc: 0.5336 - val_loss: 0.6975 - val_acc: 0.5073\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6922 - acc: 0.5327 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6923 - acc: 0.5327 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6922 - acc: 0.5318 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6921 - acc: 0.5318 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6921 - acc: 0.5345 - val_loss: 0.6972 - val_acc: 0.5064\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6921 - acc: 0.5336 - val_loss: 0.6972 - val_acc: 0.5073\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6921 - acc: 0.5336 - val_loss: 0.6971 - val_acc: 0.5073\n",
      "Train on 1100 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6920 - acc: 0.5336 - val_loss: 0.6972 - val_acc: 0.5073\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6920 - acc: 0.5318 - val_loss: 0.6973 - val_acc: 0.5082\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6919 - acc: 0.5318 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6919 - acc: 0.5327 - val_loss: 0.6971 - val_acc: 0.5073\n",
      "Epoch 5/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6918 - acc: 0.5318 - val_loss: 0.6972 - val_acc: 0.5082\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6918 - acc: 0.5318 - val_loss: 0.6971 - val_acc: 0.5073\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6917 - acc: 0.5327 - val_loss: 0.6971 - val_acc: 0.5073\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6917 - acc: 0.5309 - val_loss: 0.6972 - val_acc: 0.5082\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6917 - acc: 0.5300 - val_loss: 0.6971 - val_acc: 0.5073\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6916 - acc: 0.5300 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6916 - acc: 0.5291 - val_loss: 0.6972 - val_acc: 0.5064\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6915 - acc: 0.5291 - val_loss: 0.6971 - val_acc: 0.5073\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6915 - acc: 0.5300 - val_loss: 0.6970 - val_acc: 0.5064\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6914 - acc: 0.5318 - val_loss: 0.6968 - val_acc: 0.5064\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6915 - acc: 0.5318 - val_loss: 0.6966 - val_acc: 0.5073\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6915 - acc: 0.5309 - val_loss: 0.6967 - val_acc: 0.5082\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6913 - acc: 0.5309 - val_loss: 0.6970 - val_acc: 0.5082\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6914 - acc: 0.5282 - val_loss: 0.6972 - val_acc: 0.5082\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6913 - acc: 0.5282 - val_loss: 0.6969 - val_acc: 0.5100\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6912 - acc: 0.5309 - val_loss: 0.6966 - val_acc: 0.5091\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6913 - acc: 0.5300 - val_loss: 0.6964 - val_acc: 0.5082\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6913 - acc: 0.5291 - val_loss: 0.6965 - val_acc: 0.5082\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6912 - acc: 0.5300 - val_loss: 0.6967 - val_acc: 0.5082\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6911 - acc: 0.5300 - val_loss: 0.6968 - val_acc: 0.5082\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6910 - acc: 0.5291 - val_loss: 0.6969 - val_acc: 0.5082\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6910 - acc: 0.5291 - val_loss: 0.6968 - val_acc: 0.5073\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6910 - acc: 0.5291 - val_loss: 0.6968 - val_acc: 0.5073\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6909 - acc: 0.5300 - val_loss: 0.6967 - val_acc: 0.5082\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6909 - acc: 0.5300 - val_loss: 0.6966 - val_acc: 0.5082\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6908 - acc: 0.5291 - val_loss: 0.6967 - val_acc: 0.5082\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6908 - acc: 0.5282 - val_loss: 0.6969 - val_acc: 0.5082\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6908 - acc: 0.5273 - val_loss: 0.6969 - val_acc: 0.5082\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6908 - acc: 0.5264 - val_loss: 0.6968 - val_acc: 0.5091\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6907 - acc: 0.5273 - val_loss: 0.6965 - val_acc: 0.5091\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6907 - acc: 0.5282 - val_loss: 0.6965 - val_acc: 0.5091\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6907 - acc: 0.5264 - val_loss: 0.6967 - val_acc: 0.5091\n",
      "Epoch 37/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6906 - acc: 0.5264 - val_loss: 0.6967 - val_acc: 0.5100\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6906 - acc: 0.5273 - val_loss: 0.6967 - val_acc: 0.5109\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6905 - acc: 0.5273 - val_loss: 0.6966 - val_acc: 0.5100\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6905 - acc: 0.5273 - val_loss: 0.6967 - val_acc: 0.5118\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6904 - acc: 0.5273 - val_loss: 0.6965 - val_acc: 0.5118\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6905 - acc: 0.5300 - val_loss: 0.6964 - val_acc: 0.5118\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6905 - acc: 0.5291 - val_loss: 0.6964 - val_acc: 0.5118\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6904 - acc: 0.5291 - val_loss: 0.6964 - val_acc: 0.5118\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6904 - acc: 0.5291 - val_loss: 0.6965 - val_acc: 0.5118\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6903 - acc: 0.5291 - val_loss: 0.6967 - val_acc: 0.5118\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6904 - acc: 0.5273 - val_loss: 0.6970 - val_acc: 0.5127\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6904 - acc: 0.5255 - val_loss: 0.6969 - val_acc: 0.5127\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6903 - acc: 0.5291 - val_loss: 0.6967 - val_acc: 0.5136\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6902 - acc: 0.5300 - val_loss: 0.6965 - val_acc: 0.5127\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6901 - acc: 0.5300 - val_loss: 0.6965 - val_acc: 0.5127\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6901 - acc: 0.5282 - val_loss: 0.6965 - val_acc: 0.5127\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6900 - acc: 0.5273 - val_loss: 0.6966 - val_acc: 0.5136\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6900 - acc: 0.5273 - val_loss: 0.6965 - val_acc: 0.5127\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6900 - acc: 0.5282 - val_loss: 0.6965 - val_acc: 0.5127\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6899 - acc: 0.5291 - val_loss: 0.6967 - val_acc: 0.5127\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6899 - acc: 0.5300 - val_loss: 0.6967 - val_acc: 0.5127\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6899 - acc: 0.5318 - val_loss: 0.6967 - val_acc: 0.5127\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6898 - acc: 0.5336 - val_loss: 0.6965 - val_acc: 0.5118\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6898 - acc: 0.5345 - val_loss: 0.6964 - val_acc: 0.5118\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6898 - acc: 0.5336 - val_loss: 0.6965 - val_acc: 0.5109\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6897 - acc: 0.5327 - val_loss: 0.6967 - val_acc: 0.5118\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6897 - acc: 0.5309 - val_loss: 0.6969 - val_acc: 0.5118\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6897 - acc: 0.5327 - val_loss: 0.6968 - val_acc: 0.5118\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6896 - acc: 0.5327 - val_loss: 0.6966 - val_acc: 0.5100\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6895 - acc: 0.5327 - val_loss: 0.6967 - val_acc: 0.5091\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6895 - acc: 0.5327 - val_loss: 0.6969 - val_acc: 0.5100\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6895 - acc: 0.5336 - val_loss: 0.6971 - val_acc: 0.5100\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6895 - acc: 0.5327 - val_loss: 0.6970 - val_acc: 0.5100\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6894 - acc: 0.5345 - val_loss: 0.6969 - val_acc: 0.5100\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6893 - acc: 0.5345 - val_loss: 0.6968 - val_acc: 0.5100\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6893 - acc: 0.5345 - val_loss: 0.6968 - val_acc: 0.5100\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6892 - acc: 0.5345 - val_loss: 0.6968 - val_acc: 0.5100\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6892 - acc: 0.5345 - val_loss: 0.6968 - val_acc: 0.5100\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6891 - acc: 0.5345 - val_loss: 0.6968 - val_acc: 0.5100\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6891 - acc: 0.5345 - val_loss: 0.6969 - val_acc: 0.5100\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6891 - acc: 0.5336 - val_loss: 0.6970 - val_acc: 0.5109\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6890 - acc: 0.5355 - val_loss: 0.6969 - val_acc: 0.5100\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6890 - acc: 0.5373 - val_loss: 0.6967 - val_acc: 0.5100\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6890 - acc: 0.5373 - val_loss: 0.6968 - val_acc: 0.5055\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6890 - acc: 0.5355 - val_loss: 0.6971 - val_acc: 0.5055\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5355 - val_loss: 0.6971 - val_acc: 0.5055\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5355 - val_loss: 0.6971 - val_acc: 0.5055\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5355 - val_loss: 0.6971 - val_acc: 0.5055\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5364 - val_loss: 0.6967 - val_acc: 0.5064\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6890 - acc: 0.5373 - val_loss: 0.6967 - val_acc: 0.5064\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5373 - val_loss: 0.6969 - val_acc: 0.5064\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5373 - val_loss: 0.6969 - val_acc: 0.5064\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6888 - acc: 0.5355 - val_loss: 0.6972 - val_acc: 0.5064\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6889 - acc: 0.5355 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6887 - acc: 0.5355 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6887 - acc: 0.5364 - val_loss: 0.6970 - val_acc: 0.5064\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6887 - acc: 0.5364 - val_loss: 0.6969 - val_acc: 0.5064\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6887 - acc: 0.5382 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6888 - acc: 0.5364 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6886 - acc: 0.5364 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6887 - acc: 0.5373 - val_loss: 0.6970 - val_acc: 0.5064\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6887 - acc: 0.5373 - val_loss: 0.6970 - val_acc: 0.5064\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6886 - acc: 0.5373 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6886 - acc: 0.5364 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Train on 1100 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5364 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5364 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5373 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5364 - val_loss: 0.6972 - val_acc: 0.5064\n",
      "Epoch 5/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5364 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5382 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5382 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5382 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5382 - val_loss: 0.6969 - val_acc: 0.5064\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5373 - val_loss: 0.6969 - val_acc: 0.5064\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5382 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5382 - val_loss: 0.6973 - val_acc: 0.5055\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5382 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5373 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5364 - val_loss: 0.6978 - val_acc: 0.5055\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5345 - val_loss: 0.6978 - val_acc: 0.5055\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5336 - val_loss: 0.6980 - val_acc: 0.5055\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6885 - acc: 0.5355 - val_loss: 0.6978 - val_acc: 0.5055\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5355 - val_loss: 0.6973 - val_acc: 0.5055\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5391 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5391 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5391 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5391 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6884 - acc: 0.5355 - val_loss: 0.6979 - val_acc: 0.5064\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5373 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5391 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5391 - val_loss: 0.6970 - val_acc: 0.5073\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5382 - val_loss: 0.6973 - val_acc: 0.5064\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5391 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6883 - acc: 0.5364 - val_loss: 0.6978 - val_acc: 0.5055\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5364 - val_loss: 0.6978 - val_acc: 0.5064\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6881 - acc: 0.5391 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5400 - val_loss: 0.6971 - val_acc: 0.5064\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5391 - val_loss: 0.6972 - val_acc: 0.5064\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5382 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6882 - acc: 0.5364 - val_loss: 0.6979 - val_acc: 0.5064\n",
      "Epoch 37/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6881 - acc: 0.5364 - val_loss: 0.6979 - val_acc: 0.5064\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6881 - acc: 0.5391 - val_loss: 0.6977 - val_acc: 0.5073\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6881 - acc: 0.5400 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5400 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5373 - val_loss: 0.6977 - val_acc: 0.5073\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6975 - val_acc: 0.5073\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5391 - val_loss: 0.6974 - val_acc: 0.5064\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6977 - val_acc: 0.5073\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5382 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5373 - val_loss: 0.6978 - val_acc: 0.5064\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5373 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5382 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5382 - val_loss: 0.6979 - val_acc: 0.5055\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5373 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6878 - acc: 0.5373 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6878 - acc: 0.5382 - val_loss: 0.6976 - val_acc: 0.5064\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6974 - val_acc: 0.5055\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6977 - val_acc: 0.5064\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6980 - val_acc: 0.5055\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6878 - acc: 0.5391 - val_loss: 0.6980 - val_acc: 0.5055\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6878 - acc: 0.5391 - val_loss: 0.6979 - val_acc: 0.5064\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5400 - val_loss: 0.6975 - val_acc: 0.5055\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6976 - val_acc: 0.5055\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5400 - val_loss: 0.6981 - val_acc: 0.5055\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5391 - val_loss: 0.6987 - val_acc: 0.5055\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6881 - acc: 0.5373 - val_loss: 0.6988 - val_acc: 0.5055\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6879 - acc: 0.5373 - val_loss: 0.6984 - val_acc: 0.5055\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6880 - acc: 0.5409 - val_loss: 0.6977 - val_acc: 0.5055\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6878 - acc: 0.5400 - val_loss: 0.6976 - val_acc: 0.5055\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5409 - val_loss: 0.6979 - val_acc: 0.5064\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5400 - val_loss: 0.6983 - val_acc: 0.5055\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5391 - val_loss: 0.6983 - val_acc: 0.5055\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6878 - acc: 0.5382 - val_loss: 0.6983 - val_acc: 0.5064\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5400 - val_loss: 0.6981 - val_acc: 0.5064\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5418 - val_loss: 0.6979 - val_acc: 0.5055\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5409 - val_loss: 0.6978 - val_acc: 0.5055\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5418 - val_loss: 0.6982 - val_acc: 0.5055\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5400 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5409 - val_loss: 0.6980 - val_acc: 0.5064\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5409 - val_loss: 0.6982 - val_acc: 0.5064\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5409 - val_loss: 0.6982 - val_acc: 0.5064\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5409 - val_loss: 0.6980 - val_acc: 0.5073\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5418 - val_loss: 0.6984 - val_acc: 0.5082\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5400 - val_loss: 0.6985 - val_acc: 0.5073\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5364 - val_loss: 0.6988 - val_acc: 0.5073\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5355 - val_loss: 0.6987 - val_acc: 0.5073\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5391 - val_loss: 0.6981 - val_acc: 0.5064\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5400 - val_loss: 0.6980 - val_acc: 0.5064\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5409 - val_loss: 0.6981 - val_acc: 0.5064\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5409 - val_loss: 0.6985 - val_acc: 0.5064\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5409 - val_loss: 0.6981 - val_acc: 0.5064\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5418 - val_loss: 0.6978 - val_acc: 0.5082\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5418 - val_loss: 0.6980 - val_acc: 0.5073\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5427 - val_loss: 0.6984 - val_acc: 0.5073\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5418 - val_loss: 0.6985 - val_acc: 0.5082\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5400 - val_loss: 0.6984 - val_acc: 0.5064\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5418 - val_loss: 0.6978 - val_acc: 0.5073\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5427 - val_loss: 0.6976 - val_acc: 0.5073\n",
      "Train on 1100 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6877 - acc: 0.5436 - val_loss: 0.6977 - val_acc: 0.5073\n",
      "Epoch 2/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6876 - acc: 0.5436 - val_loss: 0.6979 - val_acc: 0.5073\n",
      "Epoch 3/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6875 - acc: 0.5418 - val_loss: 0.6985 - val_acc: 0.5073\n",
      "Epoch 4/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5409 - val_loss: 0.6986 - val_acc: 0.5073\n",
      "Epoch 5/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5409 - val_loss: 0.6984 - val_acc: 0.5064\n",
      "Epoch 6/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5409 - val_loss: 0.6983 - val_acc: 0.5064\n",
      "Epoch 7/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5427 - val_loss: 0.6983 - val_acc: 0.5073\n",
      "Epoch 8/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5418 - val_loss: 0.6984 - val_acc: 0.5064\n",
      "Epoch 9/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5418 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 10/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5418 - val_loss: 0.6983 - val_acc: 0.5073\n",
      "Epoch 11/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5409 - val_loss: 0.6985 - val_acc: 0.5064\n",
      "Epoch 12/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5409 - val_loss: 0.6987 - val_acc: 0.5073\n",
      "Epoch 13/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6874 - acc: 0.5418 - val_loss: 0.6989 - val_acc: 0.5064\n",
      "Epoch 14/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5409 - val_loss: 0.6988 - val_acc: 0.5073\n",
      "Epoch 15/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5427 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 16/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5436 - val_loss: 0.6981 - val_acc: 0.5073\n",
      "Epoch 17/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5436 - val_loss: 0.6983 - val_acc: 0.5073\n",
      "Epoch 18/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5427 - val_loss: 0.6986 - val_acc: 0.5073\n",
      "Epoch 19/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6873 - acc: 0.5418 - val_loss: 0.6988 - val_acc: 0.5064\n",
      "Epoch 20/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5409 - val_loss: 0.6986 - val_acc: 0.5091\n",
      "Epoch 21/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5418 - val_loss: 0.6983 - val_acc: 0.5082\n",
      "Epoch 22/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5418 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 23/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5427 - val_loss: 0.6982 - val_acc: 0.5091\n",
      "Epoch 24/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5418 - val_loss: 0.6985 - val_acc: 0.5100\n",
      "Epoch 25/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5427 - val_loss: 0.6981 - val_acc: 0.5082\n",
      "Epoch 26/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6872 - acc: 0.5436 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 27/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5436 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 28/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5427 - val_loss: 0.6984 - val_acc: 0.5091\n",
      "Epoch 29/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5418 - val_loss: 0.6985 - val_acc: 0.5091\n",
      "Epoch 30/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5418 - val_loss: 0.6983 - val_acc: 0.5082\n",
      "Epoch 31/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5445 - val_loss: 0.6983 - val_acc: 0.5091\n",
      "Epoch 32/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5455 - val_loss: 0.6982 - val_acc: 0.5091\n",
      "Epoch 33/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5436 - val_loss: 0.6985 - val_acc: 0.5100\n",
      "Epoch 34/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5427 - val_loss: 0.6987 - val_acc: 0.5091\n",
      "Epoch 35/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5409 - val_loss: 0.6989 - val_acc: 0.5073\n",
      "Epoch 36/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5400 - val_loss: 0.6988 - val_acc: 0.5082\n",
      "Epoch 37/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5436 - val_loss: 0.6984 - val_acc: 0.5091\n",
      "Epoch 38/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5436 - val_loss: 0.6982 - val_acc: 0.5091\n",
      "Epoch 39/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5436 - val_loss: 0.6984 - val_acc: 0.5100\n",
      "Epoch 40/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5418 - val_loss: 0.6981 - val_acc: 0.5100\n",
      "Epoch 41/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5436 - val_loss: 0.6983 - val_acc: 0.5082\n",
      "Epoch 42/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5436 - val_loss: 0.6988 - val_acc: 0.5082\n",
      "Epoch 43/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5409 - val_loss: 0.6988 - val_acc: 0.5082\n",
      "Epoch 44/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5418 - val_loss: 0.6984 - val_acc: 0.5091\n",
      "Epoch 45/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5436 - val_loss: 0.6980 - val_acc: 0.5100\n",
      "Epoch 46/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5436 - val_loss: 0.6980 - val_acc: 0.5100\n",
      "Epoch 47/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5445 - val_loss: 0.6984 - val_acc: 0.5100\n",
      "Epoch 48/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6871 - acc: 0.5436 - val_loss: 0.6990 - val_acc: 0.5082\n",
      "Epoch 49/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5400 - val_loss: 0.6992 - val_acc: 0.5064\n",
      "Epoch 50/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6870 - acc: 0.5400 - val_loss: 0.6990 - val_acc: 0.5073\n",
      "Epoch 51/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5427 - val_loss: 0.6985 - val_acc: 0.5082\n",
      "Epoch 52/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5436 - val_loss: 0.6979 - val_acc: 0.5073\n",
      "Epoch 53/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5445 - val_loss: 0.6980 - val_acc: 0.5073\n",
      "Epoch 54/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6868 - acc: 0.5445 - val_loss: 0.6983 - val_acc: 0.5082\n",
      "Epoch 55/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6868 - acc: 0.5418 - val_loss: 0.6990 - val_acc: 0.5073\n",
      "Epoch 56/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5409 - val_loss: 0.6993 - val_acc: 0.5091\n",
      "Epoch 57/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5418 - val_loss: 0.6991 - val_acc: 0.5091\n",
      "Epoch 58/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6869 - acc: 0.5418 - val_loss: 0.6990 - val_acc: 0.5091\n",
      "Epoch 59/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6868 - acc: 0.5436 - val_loss: 0.6987 - val_acc: 0.5082\n",
      "Epoch 60/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6868 - acc: 0.5427 - val_loss: 0.6988 - val_acc: 0.5082\n",
      "Epoch 61/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5427 - val_loss: 0.6985 - val_acc: 0.5091\n",
      "Epoch 62/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5436 - val_loss: 0.6983 - val_acc: 0.5091\n",
      "Epoch 63/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5455 - val_loss: 0.6982 - val_acc: 0.5091\n",
      "Epoch 64/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5445 - val_loss: 0.6983 - val_acc: 0.5100\n",
      "Epoch 65/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5445 - val_loss: 0.6982 - val_acc: 0.5091\n",
      "Epoch 66/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5455 - val_loss: 0.6983 - val_acc: 0.5100\n",
      "Epoch 67/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5464 - val_loss: 0.6980 - val_acc: 0.5082\n",
      "Epoch 68/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6867 - acc: 0.5455 - val_loss: 0.6981 - val_acc: 0.5082\n",
      "Epoch 69/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5464 - val_loss: 0.6983 - val_acc: 0.5091\n",
      "Epoch 70/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5436 - val_loss: 0.6986 - val_acc: 0.5082\n",
      "Epoch 71/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5427 - val_loss: 0.6989 - val_acc: 0.5073\n",
      "Epoch 72/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5418 - val_loss: 0.6992 - val_acc: 0.5082\n",
      "Epoch 73/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5418 - val_loss: 0.6990 - val_acc: 0.5073\n",
      "Epoch 74/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5427 - val_loss: 0.6987 - val_acc: 0.5100\n",
      "Epoch 75/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5427 - val_loss: 0.6988 - val_acc: 0.5100\n",
      "Epoch 76/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5436 - val_loss: 0.6987 - val_acc: 0.5100\n",
      "Epoch 77/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5436 - val_loss: 0.6987 - val_acc: 0.5100\n",
      "Epoch 78/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5427 - val_loss: 0.6987 - val_acc: 0.5100\n",
      "Epoch 79/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5418 - val_loss: 0.6989 - val_acc: 0.5100\n",
      "Epoch 80/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6866 - acc: 0.5418 - val_loss: 0.6992 - val_acc: 0.5091\n",
      "Epoch 81/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5409 - val_loss: 0.6991 - val_acc: 0.5082\n",
      "Epoch 82/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5418 - val_loss: 0.6990 - val_acc: 0.5082\n",
      "Epoch 83/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5418 - val_loss: 0.6990 - val_acc: 0.5091\n",
      "Epoch 84/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5427 - val_loss: 0.6988 - val_acc: 0.5100\n",
      "Epoch 85/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5418 - val_loss: 0.6988 - val_acc: 0.5100\n",
      "Epoch 86/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5436 - val_loss: 0.6983 - val_acc: 0.5091\n",
      "Epoch 87/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5436 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 88/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6865 - acc: 0.5436 - val_loss: 0.6982 - val_acc: 0.5073\n",
      "Epoch 89/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6862 - acc: 0.5427 - val_loss: 0.6987 - val_acc: 0.5100\n",
      "Epoch 90/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6863 - acc: 0.5427 - val_loss: 0.6993 - val_acc: 0.5073\n",
      "Epoch 91/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5427 - val_loss: 0.6994 - val_acc: 0.5064\n",
      "Epoch 92/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5436 - val_loss: 0.6993 - val_acc: 0.5055\n",
      "Epoch 93/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6864 - acc: 0.5436 - val_loss: 0.6991 - val_acc: 0.5055\n",
      "Epoch 94/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6862 - acc: 0.5445 - val_loss: 0.6988 - val_acc: 0.5091\n",
      "Epoch 95/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6863 - acc: 0.5445 - val_loss: 0.6983 - val_acc: 0.5100\n",
      "Epoch 96/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6862 - acc: 0.5445 - val_loss: 0.6985 - val_acc: 0.5091\n",
      "Epoch 97/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6861 - acc: 0.5455 - val_loss: 0.6987 - val_acc: 0.5100\n",
      "Epoch 98/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6861 - acc: 0.5445 - val_loss: 0.6989 - val_acc: 0.5100\n",
      "Epoch 99/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6861 - acc: 0.5427 - val_loss: 0.6992 - val_acc: 0.5073\n",
      "Epoch 100/100\n",
      "1100/1100 [==============================] - 0s - loss: 0.6862 - acc: 0.5427 - val_loss: 0.6988 - val_acc: 0.5100\n",
      "1024/1100 [==========================>...] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69881954041394323, 0.50999999978325583]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer = Input(shape=(num_features,))\n",
    "\n",
    "encoded = Dense(48, activation='relu')(input_layer)\n",
    "encoded = Dense(32, activation='relu')(encoded)\n",
    "encoded = Dense(16, activation='relu')(encoded)\n",
    "decoded = Dense(32, activation='relu')(encoded)\n",
    "decoded = Dense(48, activation='relu')(decoded)\n",
    "decoded = Dense(num_features, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(input=input_layer, output=decoded)\n",
    "encoder = Model(input=input_layer, output=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                nb_epoch=200,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test), verbose=0)\n",
    "\n",
    "encoded_train = encoder.predict(X_train)\n",
    "encoded_test = encoder.predict(X_test)\n",
    "\n",
    "encoder_input = Input(shape=(16,))\n",
    "encoded_features = Dense(8, activation='relu')(encoder_input)\n",
    "#encoded_features = Dropout(0.5)(encoded_features)\n",
    "encoded_features = Dense(2, activation='softmax')(encoded_features)\n",
    "\n",
    "encoder_classifier = Model(input=encoder_input, output=encoded_features)\n",
    "\n",
    "encoder_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for i in range(5):\n",
    "    encoder_classifier.fit(encoded_train, y_train_onehot,\n",
    "                    nb_epoch=100,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(encoded_test, y_test_onehot))\n",
    "\n",
    "encoder_classifier.evaluate(encoded_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1100 samples, validate on 1100 samples\n",
      "Epoch 1/500\n",
      "1100/1100 [==============================] - 0s - loss: 7.1161 - acc: 0.5045 - val_loss: 7.6361 - val_acc: 0.5218\n",
      "Epoch 2/500\n",
      "1100/1100 [==============================] - 0s - loss: 7.2594 - acc: 0.4936 - val_loss: 7.6371 - val_acc: 0.5218\n",
      "Epoch 3/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.9835 - acc: 0.5064 - val_loss: 7.6329 - val_acc: 0.5218\n",
      "Epoch 4/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.9086 - acc: 0.5073 - val_loss: 7.6227 - val_acc: 0.5218\n",
      "Epoch 5/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.9132 - acc: 0.5136 - val_loss: 7.6076 - val_acc: 0.5218\n",
      "Epoch 6/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.7779 - acc: 0.5145 - val_loss: 7.5855 - val_acc: 0.5218\n",
      "Epoch 7/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.8607 - acc: 0.5000 - val_loss: 7.5637 - val_acc: 0.5218\n",
      "Epoch 8/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.7654 - acc: 0.5082 - val_loss: 7.5479 - val_acc: 0.5218\n",
      "Epoch 9/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.5319 - acc: 0.5227 - val_loss: 7.5214 - val_acc: 0.5218\n",
      "Epoch 10/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.6039 - acc: 0.5145 - val_loss: 7.4957 - val_acc: 0.5218\n",
      "Epoch 11/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.7331 - acc: 0.5027 - val_loss: 7.4676 - val_acc: 0.5218\n",
      "Epoch 12/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.4814 - acc: 0.5136 - val_loss: 7.4186 - val_acc: 0.5218\n",
      "Epoch 13/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.4269 - acc: 0.5173 - val_loss: 7.3511 - val_acc: 0.5218\n",
      "Epoch 14/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.2696 - acc: 0.5191 - val_loss: 7.2643 - val_acc: 0.5218\n",
      "Epoch 15/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.2027 - acc: 0.5191 - val_loss: 7.1463 - val_acc: 0.5209\n",
      "Epoch 16/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.1032 - acc: 0.5245 - val_loss: 6.9850 - val_acc: 0.5218\n",
      "Epoch 17/500\n",
      "1100/1100 [==============================] - 0s - loss: 6.0522 - acc: 0.5173 - val_loss: 6.7348 - val_acc: 0.5218\n",
      "Epoch 18/500\n",
      "1100/1100 [==============================] - 0s - loss: 5.9934 - acc: 0.5082 - val_loss: 6.4440 - val_acc: 0.5236\n",
      "Epoch 19/500\n",
      "1100/1100 [==============================] - 0s - loss: 5.6410 - acc: 0.5236 - val_loss: 6.0623 - val_acc: 0.5309\n",
      "Epoch 20/500\n",
      "1100/1100 [==============================] - 0s - loss: 5.1412 - acc: 0.5409 - val_loss: 5.4284 - val_acc: 0.5500\n",
      "Epoch 21/500\n",
      "1100/1100 [==============================] - 0s - loss: 4.9844 - acc: 0.5445 - val_loss: 4.5232 - val_acc: 0.5736\n",
      "Epoch 22/500\n",
      "1100/1100 [==============================] - 0s - loss: 4.7363 - acc: 0.5445 - val_loss: 3.3207 - val_acc: 0.6100\n",
      "Epoch 23/500\n",
      "1100/1100 [==============================] - 0s - loss: 4.1774 - acc: 0.5609 - val_loss: 2.2033 - val_acc: 0.6409\n",
      "Epoch 24/500\n",
      "1100/1100 [==============================] - 0s - loss: 3.4570 - acc: 0.5836 - val_loss: 1.8064 - val_acc: 0.6682\n",
      "Epoch 25/500\n",
      "1100/1100 [==============================] - 0s - loss: 3.4635 - acc: 0.5636 - val_loss: 1.7583 - val_acc: 0.6664\n",
      "Epoch 26/500\n",
      "1100/1100 [==============================] - 0s - loss: 2.9338 - acc: 0.6109 - val_loss: 1.6493 - val_acc: 0.6709\n",
      "Epoch 27/500\n",
      "1100/1100 [==============================] - 0s - loss: 2.6527 - acc: 0.5918 - val_loss: 1.4883 - val_acc: 0.6791\n",
      "Epoch 28/500\n",
      "1100/1100 [==============================] - 0s - loss: 2.3573 - acc: 0.5927 - val_loss: 1.3324 - val_acc: 0.6791\n",
      "Epoch 29/500\n",
      "1100/1100 [==============================] - 0s - loss: 2.0337 - acc: 0.6055 - val_loss: 1.1896 - val_acc: 0.6709\n",
      "Epoch 30/500\n",
      "1100/1100 [==============================] - 0s - loss: 1.7702 - acc: 0.6118 - val_loss: 1.0622 - val_acc: 0.6809\n",
      "Epoch 31/500\n",
      "1100/1100 [==============================] - 0s - loss: 1.5134 - acc: 0.5973 - val_loss: 0.9501 - val_acc: 0.6818\n",
      "Epoch 32/500\n",
      "1100/1100 [==============================] - 0s - loss: 1.2868 - acc: 0.6055 - val_loss: 0.8541 - val_acc: 0.6809\n",
      "Epoch 33/500\n",
      "1100/1100 [==============================] - 0s - loss: 1.0391 - acc: 0.6282 - val_loss: 0.7814 - val_acc: 0.6655\n",
      "Epoch 34/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.9067 - acc: 0.6027 - val_loss: 0.7277 - val_acc: 0.6445\n",
      "Epoch 35/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.8178 - acc: 0.6118 - val_loss: 0.6881 - val_acc: 0.6391\n",
      "Epoch 36/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.7740 - acc: 0.6127 - val_loss: 0.6655 - val_acc: 0.6509\n",
      "Epoch 37/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.7427 - acc: 0.6036 - val_loss: 0.6542 - val_acc: 0.6455\n",
      "Epoch 38/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.7153 - acc: 0.6027 - val_loss: 0.6501 - val_acc: 0.6373\n",
      "Epoch 39/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6979 - acc: 0.5900 - val_loss: 0.6473 - val_acc: 0.6400\n",
      "Epoch 40/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6886 - acc: 0.6036 - val_loss: 0.6476 - val_acc: 0.6482\n",
      "Epoch 41/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6797 - acc: 0.5900 - val_loss: 0.6478 - val_acc: 0.6327\n",
      "Epoch 42/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6743 - acc: 0.5936 - val_loss: 0.6458 - val_acc: 0.6200\n",
      "Epoch 43/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6594 - acc: 0.6127 - val_loss: 0.6417 - val_acc: 0.6355\n",
      "Epoch 44/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6543 - acc: 0.6027 - val_loss: 0.6376 - val_acc: 0.6409\n",
      "Epoch 45/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6474 - acc: 0.6064 - val_loss: 0.6349 - val_acc: 0.6473\n",
      "Epoch 46/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6506 - acc: 0.5991 - val_loss: 0.6332 - val_acc: 0.6445\n",
      "Epoch 47/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6553 - acc: 0.5973 - val_loss: 0.6309 - val_acc: 0.6482\n",
      "Epoch 48/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6386 - acc: 0.6209 - val_loss: 0.6280 - val_acc: 0.6564\n",
      "Epoch 49/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6404 - acc: 0.6055 - val_loss: 0.6263 - val_acc: 0.6536\n",
      "Epoch 50/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6314 - acc: 0.6236 - val_loss: 0.6263 - val_acc: 0.6536\n",
      "Epoch 51/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6337 - acc: 0.6136 - val_loss: 0.6286 - val_acc: 0.6491\n",
      "Epoch 52/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6218 - acc: 0.6200 - val_loss: 0.6296 - val_acc: 0.6500\n",
      "Epoch 53/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6340 - acc: 0.6155 - val_loss: 0.6288 - val_acc: 0.6582\n",
      "Epoch 54/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6267 - acc: 0.6364 - val_loss: 0.6276 - val_acc: 0.6573\n",
      "Epoch 55/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6231 - acc: 0.6409 - val_loss: 0.6253 - val_acc: 0.6673\n",
      "Epoch 56/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6153 - acc: 0.6418 - val_loss: 0.6239 - val_acc: 0.6682\n",
      "Epoch 57/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6266 - acc: 0.6155 - val_loss: 0.6221 - val_acc: 0.6718\n",
      "Epoch 58/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6275 - acc: 0.6300 - val_loss: 0.6219 - val_acc: 0.6682\n",
      "Epoch 59/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6156 - acc: 0.6427 - val_loss: 0.6217 - val_acc: 0.6709\n",
      "Epoch 60/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6122 - acc: 0.6200 - val_loss: 0.6208 - val_acc: 0.6773\n",
      "Epoch 61/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6126 - acc: 0.6482 - val_loss: 0.6190 - val_acc: 0.6809\n",
      "Epoch 62/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6188 - acc: 0.6282 - val_loss: 0.6173 - val_acc: 0.6782\n",
      "Epoch 63/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6050 - acc: 0.6509 - val_loss: 0.6157 - val_acc: 0.6782\n",
      "Epoch 64/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6213 - acc: 0.6255 - val_loss: 0.6136 - val_acc: 0.6818\n",
      "Epoch 65/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6191 - acc: 0.6473 - val_loss: 0.6119 - val_acc: 0.6882\n",
      "Epoch 66/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6008 - acc: 0.6636 - val_loss: 0.6110 - val_acc: 0.6900\n",
      "Epoch 67/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6162 - acc: 0.6445 - val_loss: 0.6099 - val_acc: 0.6900\n",
      "Epoch 68/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6083 - acc: 0.6409 - val_loss: 0.6074 - val_acc: 0.6955\n",
      "Epoch 69/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6063 - acc: 0.6509 - val_loss: 0.6046 - val_acc: 0.6945\n",
      "Epoch 70/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5927 - acc: 0.6609 - val_loss: 0.6003 - val_acc: 0.6982\n",
      "Epoch 71/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5976 - acc: 0.6527 - val_loss: 0.5954 - val_acc: 0.7036\n",
      "Epoch 72/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6068 - acc: 0.6491 - val_loss: 0.5916 - val_acc: 0.7082\n",
      "Epoch 73/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5978 - acc: 0.6673 - val_loss: 0.5894 - val_acc: 0.7145\n",
      "Epoch 74/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5929 - acc: 0.6773 - val_loss: 0.5879 - val_acc: 0.7182\n",
      "Epoch 75/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5870 - acc: 0.6882 - val_loss: 0.5868 - val_acc: 0.7245\n",
      "Epoch 76/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5929 - acc: 0.6645 - val_loss: 0.5855 - val_acc: 0.7264\n",
      "Epoch 77/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5950 - acc: 0.6609 - val_loss: 0.5854 - val_acc: 0.7291\n",
      "Epoch 78/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5801 - acc: 0.6845 - val_loss: 0.5845 - val_acc: 0.7282\n",
      "Epoch 79/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5830 - acc: 0.6773 - val_loss: 0.5834 - val_acc: 0.7309\n",
      "Epoch 80/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5841 - acc: 0.6836 - val_loss: 0.5800 - val_acc: 0.7327\n",
      "Epoch 81/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.6015 - acc: 0.6718 - val_loss: 0.5781 - val_acc: 0.7345\n",
      "Epoch 82/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5920 - acc: 0.6736 - val_loss: 0.5785 - val_acc: 0.7318\n",
      "Epoch 83/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5796 - acc: 0.6982 - val_loss: 0.5788 - val_acc: 0.7364\n",
      "Epoch 84/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5758 - acc: 0.6918 - val_loss: 0.5781 - val_acc: 0.7436\n",
      "Epoch 85/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5732 - acc: 0.7000 - val_loss: 0.5750 - val_acc: 0.7409\n",
      "Epoch 86/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5782 - acc: 0.6809 - val_loss: 0.5717 - val_acc: 0.7409\n",
      "Epoch 87/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5707 - acc: 0.6882 - val_loss: 0.5700 - val_acc: 0.7418\n",
      "Epoch 88/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5938 - acc: 0.6809 - val_loss: 0.5694 - val_acc: 0.7436\n",
      "Epoch 89/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5754 - acc: 0.6918 - val_loss: 0.5691 - val_acc: 0.7427\n",
      "Epoch 90/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5857 - acc: 0.6745 - val_loss: 0.5686 - val_acc: 0.7482\n",
      "Epoch 91/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5699 - acc: 0.6973 - val_loss: 0.5675 - val_acc: 0.7518\n",
      "Epoch 92/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5805 - acc: 0.6909 - val_loss: 0.5662 - val_acc: 0.7545\n",
      "Epoch 93/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5746 - acc: 0.7127 - val_loss: 0.5651 - val_acc: 0.7518\n",
      "Epoch 94/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5771 - acc: 0.7045 - val_loss: 0.5636 - val_acc: 0.7536\n",
      "Epoch 95/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5629 - acc: 0.7182 - val_loss: 0.5631 - val_acc: 0.7536\n",
      "Epoch 96/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5728 - acc: 0.7064 - val_loss: 0.5604 - val_acc: 0.7564\n",
      "Epoch 97/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5719 - acc: 0.7209 - val_loss: 0.5579 - val_acc: 0.7564\n",
      "Epoch 98/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5479 - acc: 0.7364 - val_loss: 0.5553 - val_acc: 0.7636\n",
      "Epoch 99/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5741 - acc: 0.7173 - val_loss: 0.5531 - val_acc: 0.7691\n",
      "Epoch 100/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5744 - acc: 0.6991 - val_loss: 0.5524 - val_acc: 0.7645\n",
      "Epoch 101/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5638 - acc: 0.7118 - val_loss: 0.5520 - val_acc: 0.7645\n",
      "Epoch 102/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5807 - acc: 0.6873 - val_loss: 0.5526 - val_acc: 0.7655\n",
      "Epoch 103/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5597 - acc: 0.7191 - val_loss: 0.5533 - val_acc: 0.7518\n",
      "Epoch 104/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5807 - acc: 0.6964 - val_loss: 0.5534 - val_acc: 0.7509\n",
      "Epoch 105/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5584 - acc: 0.7091 - val_loss: 0.5530 - val_acc: 0.7564\n",
      "Epoch 106/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5607 - acc: 0.7045 - val_loss: 0.5520 - val_acc: 0.7573\n",
      "Epoch 107/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5603 - acc: 0.7127 - val_loss: 0.5528 - val_acc: 0.7582\n",
      "Epoch 108/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5658 - acc: 0.7045 - val_loss: 0.5538 - val_acc: 0.7582\n",
      "Epoch 109/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5712 - acc: 0.7055 - val_loss: 0.5550 - val_acc: 0.7564\n",
      "Epoch 110/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5628 - acc: 0.7045 - val_loss: 0.5536 - val_acc: 0.7582\n",
      "Epoch 111/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5613 - acc: 0.7145 - val_loss: 0.5518 - val_acc: 0.7564\n",
      "Epoch 112/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5501 - acc: 0.7109 - val_loss: 0.5496 - val_acc: 0.7600\n",
      "Epoch 113/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5498 - acc: 0.7145 - val_loss: 0.5460 - val_acc: 0.7673\n",
      "Epoch 114/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5722 - acc: 0.7145 - val_loss: 0.5449 - val_acc: 0.7709\n",
      "Epoch 115/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5588 - acc: 0.7191 - val_loss: 0.5441 - val_acc: 0.7664\n",
      "Epoch 116/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5501 - acc: 0.7191 - val_loss: 0.5414 - val_acc: 0.7691\n",
      "Epoch 117/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5566 - acc: 0.7100 - val_loss: 0.5397 - val_acc: 0.7745\n",
      "Epoch 118/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5519 - acc: 0.7191 - val_loss: 0.5400 - val_acc: 0.7691\n",
      "Epoch 119/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5451 - acc: 0.7273 - val_loss: 0.5416 - val_acc: 0.7700\n",
      "Epoch 120/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5590 - acc: 0.7109 - val_loss: 0.5423 - val_acc: 0.7664\n",
      "Epoch 121/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5437 - acc: 0.7227 - val_loss: 0.5423 - val_acc: 0.7673\n",
      "Epoch 122/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5682 - acc: 0.7082 - val_loss: 0.5426 - val_acc: 0.7655\n",
      "Epoch 123/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5436 - acc: 0.7200 - val_loss: 0.5427 - val_acc: 0.7664\n",
      "Epoch 124/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5609 - acc: 0.7127 - val_loss: 0.5433 - val_acc: 0.7655\n",
      "Epoch 125/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5555 - acc: 0.7227 - val_loss: 0.5427 - val_acc: 0.7673\n",
      "Epoch 126/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5482 - acc: 0.7200 - val_loss: 0.5413 - val_acc: 0.7655\n",
      "Epoch 127/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5486 - acc: 0.7191 - val_loss: 0.5390 - val_acc: 0.7664\n",
      "Epoch 128/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5564 - acc: 0.7127 - val_loss: 0.5375 - val_acc: 0.7636\n",
      "Epoch 129/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5469 - acc: 0.7182 - val_loss: 0.5375 - val_acc: 0.7682\n",
      "Epoch 130/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5503 - acc: 0.7145 - val_loss: 0.5369 - val_acc: 0.7655\n",
      "Epoch 131/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5260 - acc: 0.7327 - val_loss: 0.5342 - val_acc: 0.7655\n",
      "Epoch 132/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5470 - acc: 0.7173 - val_loss: 0.5331 - val_acc: 0.7655\n",
      "Epoch 133/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5411 - acc: 0.7018 - val_loss: 0.5337 - val_acc: 0.7673\n",
      "Epoch 134/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5386 - acc: 0.7382 - val_loss: 0.5332 - val_acc: 0.7664\n",
      "Epoch 135/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5482 - acc: 0.7164 - val_loss: 0.5329 - val_acc: 0.7673\n",
      "Epoch 136/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5464 - acc: 0.7264 - val_loss: 0.5329 - val_acc: 0.7691\n",
      "Epoch 137/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5438 - acc: 0.7136 - val_loss: 0.5328 - val_acc: 0.7682\n",
      "Epoch 138/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5437 - acc: 0.7309 - val_loss: 0.5324 - val_acc: 0.7682\n",
      "Epoch 139/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5469 - acc: 0.7027 - val_loss: 0.5311 - val_acc: 0.7655\n",
      "Epoch 140/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5398 - acc: 0.7227 - val_loss: 0.5300 - val_acc: 0.7664\n",
      "Epoch 141/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5400 - acc: 0.7218 - val_loss: 0.5281 - val_acc: 0.7655\n",
      "Epoch 142/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5353 - acc: 0.7391 - val_loss: 0.5286 - val_acc: 0.7718\n",
      "Epoch 143/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5446 - acc: 0.7236 - val_loss: 0.5286 - val_acc: 0.7709\n",
      "Epoch 144/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5450 - acc: 0.7318 - val_loss: 0.5288 - val_acc: 0.7709\n",
      "Epoch 145/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5424 - acc: 0.7182 - val_loss: 0.5291 - val_acc: 0.7709\n",
      "Epoch 146/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5221 - acc: 0.7200 - val_loss: 0.5270 - val_acc: 0.7673\n",
      "Epoch 147/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5293 - acc: 0.7200 - val_loss: 0.5252 - val_acc: 0.7664\n",
      "Epoch 148/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5334 - acc: 0.7327 - val_loss: 0.5228 - val_acc: 0.7655\n",
      "Epoch 149/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5512 - acc: 0.7273 - val_loss: 0.5227 - val_acc: 0.7727\n",
      "Epoch 150/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5539 - acc: 0.7155 - val_loss: 0.5238 - val_acc: 0.7727\n",
      "Epoch 151/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5380 - acc: 0.7336 - val_loss: 0.5258 - val_acc: 0.7745\n",
      "Epoch 152/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5341 - acc: 0.7100 - val_loss: 0.5280 - val_acc: 0.7682\n",
      "Epoch 153/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5238 - acc: 0.7482 - val_loss: 0.5282 - val_acc: 0.7700\n",
      "Epoch 154/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5317 - acc: 0.7355 - val_loss: 0.5276 - val_acc: 0.7691\n",
      "Epoch 155/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5422 - acc: 0.7264 - val_loss: 0.5262 - val_acc: 0.7718\n",
      "Epoch 156/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5356 - acc: 0.7200 - val_loss: 0.5247 - val_acc: 0.7773\n",
      "Epoch 157/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5226 - acc: 0.7309 - val_loss: 0.5225 - val_acc: 0.7773\n",
      "Epoch 158/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5389 - acc: 0.7182 - val_loss: 0.5213 - val_acc: 0.7764\n",
      "Epoch 159/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5349 - acc: 0.7136 - val_loss: 0.5201 - val_acc: 0.7773\n",
      "Epoch 160/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5319 - acc: 0.7209 - val_loss: 0.5176 - val_acc: 0.7745\n",
      "Epoch 161/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5368 - acc: 0.7118 - val_loss: 0.5176 - val_acc: 0.7718\n",
      "Epoch 162/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5368 - acc: 0.7309 - val_loss: 0.5176 - val_acc: 0.7745\n",
      "Epoch 163/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5150 - acc: 0.7418 - val_loss: 0.5166 - val_acc: 0.7782\n",
      "Epoch 164/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5384 - acc: 0.7345 - val_loss: 0.5175 - val_acc: 0.7809\n",
      "Epoch 165/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5295 - acc: 0.7200 - val_loss: 0.5182 - val_acc: 0.7782\n",
      "Epoch 166/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5232 - acc: 0.7545 - val_loss: 0.5183 - val_acc: 0.7755\n",
      "Epoch 167/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5340 - acc: 0.7364 - val_loss: 0.5174 - val_acc: 0.7755\n",
      "Epoch 168/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5114 - acc: 0.7364 - val_loss: 0.5169 - val_acc: 0.7773\n",
      "Epoch 169/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5296 - acc: 0.7309 - val_loss: 0.5162 - val_acc: 0.7791\n",
      "Epoch 170/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5174 - acc: 0.7364 - val_loss: 0.5150 - val_acc: 0.7791\n",
      "Epoch 171/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5246 - acc: 0.7382 - val_loss: 0.5138 - val_acc: 0.7800\n",
      "Epoch 172/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5335 - acc: 0.7282 - val_loss: 0.5133 - val_acc: 0.7827\n",
      "Epoch 173/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5149 - acc: 0.7473 - val_loss: 0.5135 - val_acc: 0.7791\n",
      "Epoch 174/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5277 - acc: 0.7373 - val_loss: 0.5147 - val_acc: 0.7745\n",
      "Epoch 175/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5241 - acc: 0.7318 - val_loss: 0.5153 - val_acc: 0.7745\n",
      "Epoch 176/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5320 - acc: 0.7300 - val_loss: 0.5157 - val_acc: 0.7773\n",
      "Epoch 177/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5205 - acc: 0.7327 - val_loss: 0.5156 - val_acc: 0.7773\n",
      "Epoch 178/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5241 - acc: 0.7436 - val_loss: 0.5161 - val_acc: 0.7764\n",
      "Epoch 179/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5091 - acc: 0.7400 - val_loss: 0.5146 - val_acc: 0.7809\n",
      "Epoch 180/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5172 - acc: 0.7427 - val_loss: 0.5120 - val_acc: 0.7791\n",
      "Epoch 181/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5355 - acc: 0.7200 - val_loss: 0.5105 - val_acc: 0.7809\n",
      "Epoch 182/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5191 - acc: 0.7364 - val_loss: 0.5085 - val_acc: 0.7845\n",
      "Epoch 183/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5200 - acc: 0.7291 - val_loss: 0.5076 - val_acc: 0.7855\n",
      "Epoch 184/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5068 - acc: 0.7500 - val_loss: 0.5074 - val_acc: 0.7855\n",
      "Epoch 185/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5270 - acc: 0.7218 - val_loss: 0.5065 - val_acc: 0.7855\n",
      "Epoch 186/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5195 - acc: 0.7300 - val_loss: 0.5076 - val_acc: 0.7800\n",
      "Epoch 187/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5318 - acc: 0.7291 - val_loss: 0.5092 - val_acc: 0.7836\n",
      "Epoch 188/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5204 - acc: 0.7327 - val_loss: 0.5113 - val_acc: 0.7773\n",
      "Epoch 189/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5185 - acc: 0.7300 - val_loss: 0.5114 - val_acc: 0.7782\n",
      "Epoch 190/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5286 - acc: 0.7355 - val_loss: 0.5121 - val_acc: 0.7782\n",
      "Epoch 191/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5206 - acc: 0.7282 - val_loss: 0.5115 - val_acc: 0.7764\n",
      "Epoch 192/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5284 - acc: 0.7264 - val_loss: 0.5123 - val_acc: 0.7791\n",
      "Epoch 193/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5028 - acc: 0.7427 - val_loss: 0.5117 - val_acc: 0.7809\n",
      "Epoch 194/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5194 - acc: 0.7300 - val_loss: 0.5094 - val_acc: 0.7818\n",
      "Epoch 195/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5074 - acc: 0.7491 - val_loss: 0.5072 - val_acc: 0.7809\n",
      "Epoch 196/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5287 - acc: 0.7318 - val_loss: 0.5060 - val_acc: 0.7809\n",
      "Epoch 197/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5055 - acc: 0.7391 - val_loss: 0.5051 - val_acc: 0.7827\n",
      "Epoch 198/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5085 - acc: 0.7255 - val_loss: 0.5048 - val_acc: 0.7836\n",
      "Epoch 199/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5041 - acc: 0.7373 - val_loss: 0.5042 - val_acc: 0.7845\n",
      "Epoch 200/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5049 - acc: 0.7491 - val_loss: 0.5035 - val_acc: 0.7873\n",
      "Epoch 201/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5206 - acc: 0.7291 - val_loss: 0.5031 - val_acc: 0.7845\n",
      "Epoch 202/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5210 - acc: 0.7255 - val_loss: 0.5035 - val_acc: 0.7827\n",
      "Epoch 203/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5106 - acc: 0.7364 - val_loss: 0.5051 - val_acc: 0.7827\n",
      "Epoch 204/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5126 - acc: 0.7364 - val_loss: 0.5057 - val_acc: 0.7791\n",
      "Epoch 205/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5103 - acc: 0.7355 - val_loss: 0.5065 - val_acc: 0.7800\n",
      "Epoch 206/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5074 - acc: 0.7482 - val_loss: 0.5076 - val_acc: 0.7827\n",
      "Epoch 207/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5052 - acc: 0.7464 - val_loss: 0.5085 - val_acc: 0.7809\n",
      "Epoch 208/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4993 - acc: 0.7509 - val_loss: 0.5066 - val_acc: 0.7818\n",
      "Epoch 209/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5169 - acc: 0.7345 - val_loss: 0.5041 - val_acc: 0.7818\n",
      "Epoch 210/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4978 - acc: 0.7318 - val_loss: 0.5028 - val_acc: 0.7782\n",
      "Epoch 211/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5090 - acc: 0.7327 - val_loss: 0.5021 - val_acc: 0.7782\n",
      "Epoch 212/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5141 - acc: 0.7236 - val_loss: 0.5008 - val_acc: 0.7800\n",
      "Epoch 213/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5034 - acc: 0.7391 - val_loss: 0.4986 - val_acc: 0.7836\n",
      "Epoch 214/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4905 - acc: 0.7445 - val_loss: 0.4971 - val_acc: 0.7864\n",
      "Epoch 215/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5009 - acc: 0.7427 - val_loss: 0.4979 - val_acc: 0.7800\n",
      "Epoch 216/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5126 - acc: 0.7427 - val_loss: 0.4992 - val_acc: 0.7773\n",
      "Epoch 217/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5013 - acc: 0.7418 - val_loss: 0.4979 - val_acc: 0.7764\n",
      "Epoch 218/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5123 - acc: 0.7309 - val_loss: 0.4993 - val_acc: 0.7700\n",
      "Epoch 219/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5045 - acc: 0.7355 - val_loss: 0.5027 - val_acc: 0.7700\n",
      "Epoch 220/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4915 - acc: 0.7473 - val_loss: 0.5039 - val_acc: 0.7691\n",
      "Epoch 221/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5116 - acc: 0.7373 - val_loss: 0.5025 - val_acc: 0.7709\n",
      "Epoch 222/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4997 - acc: 0.7436 - val_loss: 0.5020 - val_acc: 0.7718\n",
      "Epoch 223/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4963 - acc: 0.7527 - val_loss: 0.4997 - val_acc: 0.7755\n",
      "Epoch 224/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5130 - acc: 0.7382 - val_loss: 0.4997 - val_acc: 0.7727\n",
      "Epoch 225/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5061 - acc: 0.7391 - val_loss: 0.4987 - val_acc: 0.7718\n",
      "Epoch 226/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5082 - acc: 0.7573 - val_loss: 0.4984 - val_acc: 0.7709\n",
      "Epoch 227/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4824 - acc: 0.7627 - val_loss: 0.4975 - val_acc: 0.7736\n",
      "Epoch 228/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4849 - acc: 0.7573 - val_loss: 0.4972 - val_acc: 0.7736\n",
      "Epoch 229/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4977 - acc: 0.7518 - val_loss: 0.4981 - val_acc: 0.7755\n",
      "Epoch 230/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5038 - acc: 0.7518 - val_loss: 0.4956 - val_acc: 0.7755\n",
      "Epoch 231/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4906 - acc: 0.7645 - val_loss: 0.4938 - val_acc: 0.7736\n",
      "Epoch 232/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4988 - acc: 0.7391 - val_loss: 0.4936 - val_acc: 0.7782\n",
      "Epoch 233/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5069 - acc: 0.7373 - val_loss: 0.4931 - val_acc: 0.7755\n",
      "Epoch 234/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5011 - acc: 0.7418 - val_loss: 0.4949 - val_acc: 0.7745\n",
      "Epoch 235/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4893 - acc: 0.7527 - val_loss: 0.4937 - val_acc: 0.7773\n",
      "Epoch 236/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4961 - acc: 0.7482 - val_loss: 0.4918 - val_acc: 0.7755\n",
      "Epoch 237/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5022 - acc: 0.7391 - val_loss: 0.4902 - val_acc: 0.7736\n",
      "Epoch 238/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4950 - acc: 0.7455 - val_loss: 0.4908 - val_acc: 0.7727\n",
      "Epoch 239/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4899 - acc: 0.7391 - val_loss: 0.4931 - val_acc: 0.7709\n",
      "Epoch 240/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5004 - acc: 0.7518 - val_loss: 0.4934 - val_acc: 0.7727\n",
      "Epoch 241/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4847 - acc: 0.7536 - val_loss: 0.4931 - val_acc: 0.7736\n",
      "Epoch 242/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5116 - acc: 0.7418 - val_loss: 0.4928 - val_acc: 0.7718\n",
      "Epoch 243/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4890 - acc: 0.7555 - val_loss: 0.4948 - val_acc: 0.7736\n",
      "Epoch 244/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4827 - acc: 0.7555 - val_loss: 0.4951 - val_acc: 0.7736\n",
      "Epoch 245/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4848 - acc: 0.7645 - val_loss: 0.4934 - val_acc: 0.7736\n",
      "Epoch 246/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4992 - acc: 0.7418 - val_loss: 0.4914 - val_acc: 0.7718\n",
      "Epoch 247/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4875 - acc: 0.7573 - val_loss: 0.4895 - val_acc: 0.7745\n",
      "Epoch 248/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4856 - acc: 0.7600 - val_loss: 0.4884 - val_acc: 0.7727\n",
      "Epoch 249/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.5026 - acc: 0.7509 - val_loss: 0.4898 - val_acc: 0.7718\n",
      "Epoch 250/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4882 - acc: 0.7518 - val_loss: 0.4908 - val_acc: 0.7709\n",
      "Epoch 251/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4995 - acc: 0.7509 - val_loss: 0.4918 - val_acc: 0.7736\n",
      "Epoch 252/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4790 - acc: 0.7636 - val_loss: 0.4924 - val_acc: 0.7736\n",
      "Epoch 253/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4896 - acc: 0.7518 - val_loss: 0.4905 - val_acc: 0.7727\n",
      "Epoch 254/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4863 - acc: 0.7545 - val_loss: 0.4875 - val_acc: 0.7736\n",
      "Epoch 255/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4818 - acc: 0.7536 - val_loss: 0.4875 - val_acc: 0.7727\n",
      "Epoch 256/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4912 - acc: 0.7500 - val_loss: 0.4866 - val_acc: 0.7718\n",
      "Epoch 257/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4894 - acc: 0.7482 - val_loss: 0.4857 - val_acc: 0.7709\n",
      "Epoch 258/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4916 - acc: 0.7427 - val_loss: 0.4860 - val_acc: 0.7718\n",
      "Epoch 259/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4871 - acc: 0.7436 - val_loss: 0.4849 - val_acc: 0.7745\n",
      "Epoch 260/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4851 - acc: 0.7527 - val_loss: 0.4872 - val_acc: 0.7745\n",
      "Epoch 261/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4857 - acc: 0.7527 - val_loss: 0.4876 - val_acc: 0.7800\n",
      "Epoch 262/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4905 - acc: 0.7509 - val_loss: 0.4895 - val_acc: 0.7782\n",
      "Epoch 263/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4805 - acc: 0.7609 - val_loss: 0.4897 - val_acc: 0.7791\n",
      "Epoch 264/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4922 - acc: 0.7564 - val_loss: 0.4887 - val_acc: 0.7773\n",
      "Epoch 265/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4710 - acc: 0.7609 - val_loss: 0.4869 - val_acc: 0.7755\n",
      "Epoch 266/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4795 - acc: 0.7618 - val_loss: 0.4860 - val_acc: 0.7773\n",
      "Epoch 267/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4966 - acc: 0.7555 - val_loss: 0.4843 - val_acc: 0.7773\n",
      "Epoch 268/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4822 - acc: 0.7682 - val_loss: 0.4849 - val_acc: 0.7773\n",
      "Epoch 269/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4731 - acc: 0.7691 - val_loss: 0.4853 - val_acc: 0.7800\n",
      "Epoch 270/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4850 - acc: 0.7482 - val_loss: 0.4852 - val_acc: 0.7764\n",
      "Epoch 271/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4841 - acc: 0.7536 - val_loss: 0.4874 - val_acc: 0.7736\n",
      "Epoch 272/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4825 - acc: 0.7473 - val_loss: 0.4887 - val_acc: 0.7727\n",
      "Epoch 273/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4711 - acc: 0.7509 - val_loss: 0.4890 - val_acc: 0.7764\n",
      "Epoch 274/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4950 - acc: 0.7518 - val_loss: 0.4874 - val_acc: 0.7755\n",
      "Epoch 275/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4856 - acc: 0.7664 - val_loss: 0.4881 - val_acc: 0.7791\n",
      "Epoch 276/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4833 - acc: 0.7600 - val_loss: 0.4877 - val_acc: 0.7791\n",
      "Epoch 277/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4747 - acc: 0.7573 - val_loss: 0.4872 - val_acc: 0.7800\n",
      "Epoch 278/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4782 - acc: 0.7545 - val_loss: 0.4862 - val_acc: 0.7782\n",
      "Epoch 279/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4798 - acc: 0.7482 - val_loss: 0.4848 - val_acc: 0.7745\n",
      "Epoch 280/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4882 - acc: 0.7564 - val_loss: 0.4833 - val_acc: 0.7782\n",
      "Epoch 281/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4709 - acc: 0.7709 - val_loss: 0.4811 - val_acc: 0.7809\n",
      "Epoch 282/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4759 - acc: 0.7700 - val_loss: 0.4802 - val_acc: 0.7764\n",
      "Epoch 283/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4734 - acc: 0.7755 - val_loss: 0.4815 - val_acc: 0.7745\n",
      "Epoch 284/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4708 - acc: 0.7700 - val_loss: 0.4811 - val_acc: 0.7727\n",
      "Epoch 285/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4922 - acc: 0.7527 - val_loss: 0.4823 - val_acc: 0.7736\n",
      "Epoch 286/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4811 - acc: 0.7627 - val_loss: 0.4844 - val_acc: 0.7736\n",
      "Epoch 287/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4810 - acc: 0.7591 - val_loss: 0.4855 - val_acc: 0.7745\n",
      "Epoch 288/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4746 - acc: 0.7655 - val_loss: 0.4859 - val_acc: 0.7718\n",
      "Epoch 289/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4733 - acc: 0.7745 - val_loss: 0.4831 - val_acc: 0.7736\n",
      "Epoch 290/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4905 - acc: 0.7627 - val_loss: 0.4816 - val_acc: 0.7736\n",
      "Epoch 291/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4719 - acc: 0.7709 - val_loss: 0.4793 - val_acc: 0.7764\n",
      "Epoch 292/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4673 - acc: 0.7736 - val_loss: 0.4770 - val_acc: 0.7755\n",
      "Epoch 293/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4837 - acc: 0.7500 - val_loss: 0.4764 - val_acc: 0.7782\n",
      "Epoch 294/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4760 - acc: 0.7527 - val_loss: 0.4761 - val_acc: 0.7791\n",
      "Epoch 295/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4900 - acc: 0.7564 - val_loss: 0.4788 - val_acc: 0.7782\n",
      "Epoch 296/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4718 - acc: 0.7564 - val_loss: 0.4822 - val_acc: 0.7718\n",
      "Epoch 297/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4796 - acc: 0.7582 - val_loss: 0.4831 - val_acc: 0.7755\n",
      "Epoch 298/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4948 - acc: 0.7582 - val_loss: 0.4827 - val_acc: 0.7755\n",
      "Epoch 299/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4803 - acc: 0.7582 - val_loss: 0.4820 - val_acc: 0.7755\n",
      "Epoch 300/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4739 - acc: 0.7645 - val_loss: 0.4814 - val_acc: 0.7682\n",
      "Epoch 301/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4790 - acc: 0.7600 - val_loss: 0.4819 - val_acc: 0.7691\n",
      "Epoch 302/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4726 - acc: 0.7609 - val_loss: 0.4814 - val_acc: 0.7700\n",
      "Epoch 303/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4726 - acc: 0.7582 - val_loss: 0.4803 - val_acc: 0.7718\n",
      "Epoch 304/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4678 - acc: 0.7636 - val_loss: 0.4811 - val_acc: 0.7718\n",
      "Epoch 305/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4679 - acc: 0.7645 - val_loss: 0.4834 - val_acc: 0.7745\n",
      "Epoch 306/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4831 - acc: 0.7573 - val_loss: 0.4835 - val_acc: 0.7745\n",
      "Epoch 307/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4680 - acc: 0.7673 - val_loss: 0.4853 - val_acc: 0.7718\n",
      "Epoch 308/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4829 - acc: 0.7591 - val_loss: 0.4873 - val_acc: 0.7700\n",
      "Epoch 309/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4833 - acc: 0.7700 - val_loss: 0.4857 - val_acc: 0.7664\n",
      "Epoch 310/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4688 - acc: 0.7582 - val_loss: 0.4839 - val_acc: 0.7627\n",
      "Epoch 311/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4737 - acc: 0.7655 - val_loss: 0.4833 - val_acc: 0.7655\n",
      "Epoch 312/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4793 - acc: 0.7745 - val_loss: 0.4829 - val_acc: 0.7682\n",
      "Epoch 313/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4809 - acc: 0.7700 - val_loss: 0.4827 - val_acc: 0.7727\n",
      "Epoch 314/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4563 - acc: 0.7682 - val_loss: 0.4822 - val_acc: 0.7709\n",
      "Epoch 315/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4646 - acc: 0.7664 - val_loss: 0.4801 - val_acc: 0.7745\n",
      "Epoch 316/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4729 - acc: 0.7627 - val_loss: 0.4802 - val_acc: 0.7709\n",
      "Epoch 317/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4747 - acc: 0.7700 - val_loss: 0.4783 - val_acc: 0.7718\n",
      "Epoch 318/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4625 - acc: 0.7709 - val_loss: 0.4756 - val_acc: 0.7709\n",
      "Epoch 319/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4826 - acc: 0.7591 - val_loss: 0.4761 - val_acc: 0.7709\n",
      "Epoch 320/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4545 - acc: 0.7709 - val_loss: 0.4775 - val_acc: 0.7682\n",
      "Epoch 321/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4698 - acc: 0.7527 - val_loss: 0.4799 - val_acc: 0.7655\n",
      "Epoch 322/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4757 - acc: 0.7700 - val_loss: 0.4805 - val_acc: 0.7691\n",
      "Epoch 323/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4651 - acc: 0.7582 - val_loss: 0.4804 - val_acc: 0.7673\n",
      "Epoch 324/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4657 - acc: 0.7600 - val_loss: 0.4799 - val_acc: 0.7691\n",
      "Epoch 325/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4663 - acc: 0.7582 - val_loss: 0.4776 - val_acc: 0.7691\n",
      "Epoch 326/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4727 - acc: 0.7455 - val_loss: 0.4762 - val_acc: 0.7718\n",
      "Epoch 327/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4721 - acc: 0.7591 - val_loss: 0.4750 - val_acc: 0.7745\n",
      "Epoch 328/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4746 - acc: 0.7591 - val_loss: 0.4756 - val_acc: 0.7727\n",
      "Epoch 329/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4654 - acc: 0.7718 - val_loss: 0.4764 - val_acc: 0.7691\n",
      "Epoch 330/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4701 - acc: 0.7664 - val_loss: 0.4772 - val_acc: 0.7645\n",
      "Epoch 331/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4643 - acc: 0.7691 - val_loss: 0.4801 - val_acc: 0.7627\n",
      "Epoch 332/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4759 - acc: 0.7655 - val_loss: 0.4810 - val_acc: 0.7636\n",
      "Epoch 333/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4698 - acc: 0.7655 - val_loss: 0.4815 - val_acc: 0.7664\n",
      "Epoch 334/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4630 - acc: 0.7682 - val_loss: 0.4822 - val_acc: 0.7691\n",
      "Epoch 335/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4639 - acc: 0.7745 - val_loss: 0.4816 - val_acc: 0.7682\n",
      "Epoch 336/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4699 - acc: 0.7609 - val_loss: 0.4803 - val_acc: 0.7700\n",
      "Epoch 337/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4810 - acc: 0.7573 - val_loss: 0.4810 - val_acc: 0.7691\n",
      "Epoch 338/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4676 - acc: 0.7573 - val_loss: 0.4805 - val_acc: 0.7682\n",
      "Epoch 339/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4762 - acc: 0.7645 - val_loss: 0.4800 - val_acc: 0.7709\n",
      "Epoch 340/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4774 - acc: 0.7545 - val_loss: 0.4803 - val_acc: 0.7709\n",
      "Epoch 341/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4754 - acc: 0.7682 - val_loss: 0.4814 - val_acc: 0.7645\n",
      "Epoch 342/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4822 - acc: 0.7464 - val_loss: 0.4817 - val_acc: 0.7627\n",
      "Epoch 343/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4595 - acc: 0.7700 - val_loss: 0.4803 - val_acc: 0.7655\n",
      "Epoch 344/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4618 - acc: 0.7691 - val_loss: 0.4774 - val_acc: 0.7700\n",
      "Epoch 345/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4573 - acc: 0.7645 - val_loss: 0.4755 - val_acc: 0.7709\n",
      "Epoch 346/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4705 - acc: 0.7555 - val_loss: 0.4752 - val_acc: 0.7727\n",
      "Epoch 347/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4639 - acc: 0.7591 - val_loss: 0.4761 - val_acc: 0.7718\n",
      "Epoch 348/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4836 - acc: 0.7591 - val_loss: 0.4780 - val_acc: 0.7691\n",
      "Epoch 349/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4754 - acc: 0.7582 - val_loss: 0.4773 - val_acc: 0.7655\n",
      "Epoch 350/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4643 - acc: 0.7600 - val_loss: 0.4756 - val_acc: 0.7673\n",
      "Epoch 351/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4724 - acc: 0.7536 - val_loss: 0.4747 - val_acc: 0.7700\n",
      "Epoch 352/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4791 - acc: 0.7536 - val_loss: 0.4754 - val_acc: 0.7700\n",
      "Epoch 353/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4599 - acc: 0.7791 - val_loss: 0.4767 - val_acc: 0.7700\n",
      "Epoch 354/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4550 - acc: 0.7700 - val_loss: 0.4758 - val_acc: 0.7709\n",
      "Epoch 355/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4798 - acc: 0.7673 - val_loss: 0.4755 - val_acc: 0.7682\n",
      "Epoch 356/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4742 - acc: 0.7618 - val_loss: 0.4744 - val_acc: 0.7700\n",
      "Epoch 357/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4671 - acc: 0.7727 - val_loss: 0.4746 - val_acc: 0.7682\n",
      "Epoch 358/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4651 - acc: 0.7518 - val_loss: 0.4754 - val_acc: 0.7700\n",
      "Epoch 359/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4621 - acc: 0.7755 - val_loss: 0.4760 - val_acc: 0.7691\n",
      "Epoch 360/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4538 - acc: 0.7800 - val_loss: 0.4751 - val_acc: 0.7736\n",
      "Epoch 361/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4656 - acc: 0.7691 - val_loss: 0.4741 - val_acc: 0.7691\n",
      "Epoch 362/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4705 - acc: 0.7755 - val_loss: 0.4737 - val_acc: 0.7682\n",
      "Epoch 363/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4769 - acc: 0.7636 - val_loss: 0.4735 - val_acc: 0.7691\n",
      "Epoch 364/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4541 - acc: 0.7700 - val_loss: 0.4740 - val_acc: 0.7655\n",
      "Epoch 365/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4691 - acc: 0.7664 - val_loss: 0.4739 - val_acc: 0.7636\n",
      "Epoch 366/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4698 - acc: 0.7718 - val_loss: 0.4726 - val_acc: 0.7709\n",
      "Epoch 367/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4566 - acc: 0.7709 - val_loss: 0.4713 - val_acc: 0.7709\n",
      "Epoch 368/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4704 - acc: 0.7655 - val_loss: 0.4710 - val_acc: 0.7682\n",
      "Epoch 369/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4620 - acc: 0.7564 - val_loss: 0.4714 - val_acc: 0.7691\n",
      "Epoch 370/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4758 - acc: 0.7582 - val_loss: 0.4749 - val_acc: 0.7673\n",
      "Epoch 371/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4725 - acc: 0.7673 - val_loss: 0.4768 - val_acc: 0.7682\n",
      "Epoch 372/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4704 - acc: 0.7527 - val_loss: 0.4777 - val_acc: 0.7655\n",
      "Epoch 373/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4706 - acc: 0.7555 - val_loss: 0.4758 - val_acc: 0.7682\n",
      "Epoch 374/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4590 - acc: 0.7600 - val_loss: 0.4728 - val_acc: 0.7664\n",
      "Epoch 375/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4664 - acc: 0.7582 - val_loss: 0.4726 - val_acc: 0.7664\n",
      "Epoch 376/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4830 - acc: 0.7527 - val_loss: 0.4726 - val_acc: 0.7673\n",
      "Epoch 377/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4756 - acc: 0.7527 - val_loss: 0.4725 - val_acc: 0.7645\n",
      "Epoch 378/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4475 - acc: 0.7818 - val_loss: 0.4736 - val_acc: 0.7627\n",
      "Epoch 379/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4733 - acc: 0.7545 - val_loss: 0.4751 - val_acc: 0.7682\n",
      "Epoch 380/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4671 - acc: 0.7618 - val_loss: 0.4771 - val_acc: 0.7700\n",
      "Epoch 381/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4658 - acc: 0.7664 - val_loss: 0.4768 - val_acc: 0.7682\n",
      "Epoch 382/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4683 - acc: 0.7600 - val_loss: 0.4768 - val_acc: 0.7682\n",
      "Epoch 383/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4697 - acc: 0.7564 - val_loss: 0.4750 - val_acc: 0.7682\n",
      "Epoch 384/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4736 - acc: 0.7582 - val_loss: 0.4751 - val_acc: 0.7645\n",
      "Epoch 385/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4563 - acc: 0.7736 - val_loss: 0.4734 - val_acc: 0.7691\n",
      "Epoch 386/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4714 - acc: 0.7627 - val_loss: 0.4708 - val_acc: 0.7664\n",
      "Epoch 387/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4681 - acc: 0.7600 - val_loss: 0.4694 - val_acc: 0.7664\n",
      "Epoch 388/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4579 - acc: 0.7782 - val_loss: 0.4702 - val_acc: 0.7655\n",
      "Epoch 389/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4677 - acc: 0.7682 - val_loss: 0.4714 - val_acc: 0.7645\n",
      "Epoch 390/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4571 - acc: 0.7736 - val_loss: 0.4711 - val_acc: 0.7655\n",
      "Epoch 391/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4587 - acc: 0.7573 - val_loss: 0.4701 - val_acc: 0.7655\n",
      "Epoch 392/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4659 - acc: 0.7555 - val_loss: 0.4697 - val_acc: 0.7673\n",
      "Epoch 393/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4661 - acc: 0.7664 - val_loss: 0.4701 - val_acc: 0.7664\n",
      "Epoch 394/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4671 - acc: 0.7627 - val_loss: 0.4703 - val_acc: 0.7636\n",
      "Epoch 395/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4725 - acc: 0.7591 - val_loss: 0.4716 - val_acc: 0.7636\n",
      "Epoch 396/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4598 - acc: 0.7645 - val_loss: 0.4714 - val_acc: 0.7627\n",
      "Epoch 397/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4750 - acc: 0.7600 - val_loss: 0.4705 - val_acc: 0.7691\n",
      "Epoch 398/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4751 - acc: 0.7536 - val_loss: 0.4697 - val_acc: 0.7709\n",
      "Epoch 399/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4653 - acc: 0.7627 - val_loss: 0.4688 - val_acc: 0.7691\n",
      "Epoch 400/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4572 - acc: 0.7718 - val_loss: 0.4688 - val_acc: 0.7664\n",
      "Epoch 401/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4680 - acc: 0.7655 - val_loss: 0.4719 - val_acc: 0.7645\n",
      "Epoch 402/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4621 - acc: 0.7627 - val_loss: 0.4727 - val_acc: 0.7636\n",
      "Epoch 403/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4696 - acc: 0.7664 - val_loss: 0.4722 - val_acc: 0.7664\n",
      "Epoch 404/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4750 - acc: 0.7591 - val_loss: 0.4724 - val_acc: 0.7655\n",
      "Epoch 405/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4524 - acc: 0.7718 - val_loss: 0.4713 - val_acc: 0.7673\n",
      "Epoch 406/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4535 - acc: 0.7618 - val_loss: 0.4719 - val_acc: 0.7673\n",
      "Epoch 407/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4586 - acc: 0.7764 - val_loss: 0.4710 - val_acc: 0.7645\n",
      "Epoch 408/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4654 - acc: 0.7655 - val_loss: 0.4706 - val_acc: 0.7673\n",
      "Epoch 409/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4555 - acc: 0.7709 - val_loss: 0.4712 - val_acc: 0.7673\n",
      "Epoch 410/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4451 - acc: 0.7791 - val_loss: 0.4708 - val_acc: 0.7691\n",
      "Epoch 411/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4665 - acc: 0.7609 - val_loss: 0.4697 - val_acc: 0.7691\n",
      "Epoch 412/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4707 - acc: 0.7591 - val_loss: 0.4695 - val_acc: 0.7673\n",
      "Epoch 413/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4569 - acc: 0.7709 - val_loss: 0.4676 - val_acc: 0.7645\n",
      "Epoch 414/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4602 - acc: 0.7664 - val_loss: 0.4683 - val_acc: 0.7664\n",
      "Epoch 415/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4692 - acc: 0.7627 - val_loss: 0.4691 - val_acc: 0.7655\n",
      "Epoch 416/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4680 - acc: 0.7527 - val_loss: 0.4708 - val_acc: 0.7636\n",
      "Epoch 417/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4612 - acc: 0.7709 - val_loss: 0.4693 - val_acc: 0.7645\n",
      "Epoch 418/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4615 - acc: 0.7800 - val_loss: 0.4683 - val_acc: 0.7673\n",
      "Epoch 419/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4709 - acc: 0.7636 - val_loss: 0.4698 - val_acc: 0.7682\n",
      "Epoch 420/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4602 - acc: 0.7745 - val_loss: 0.4713 - val_acc: 0.7682\n",
      "Epoch 421/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4637 - acc: 0.7755 - val_loss: 0.4704 - val_acc: 0.7655\n",
      "Epoch 422/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4656 - acc: 0.7636 - val_loss: 0.4699 - val_acc: 0.7655\n",
      "Epoch 423/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4702 - acc: 0.7609 - val_loss: 0.4683 - val_acc: 0.7655\n",
      "Epoch 424/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4529 - acc: 0.7636 - val_loss: 0.4668 - val_acc: 0.7655\n",
      "Epoch 425/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4636 - acc: 0.7736 - val_loss: 0.4673 - val_acc: 0.7645\n",
      "Epoch 426/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4689 - acc: 0.7536 - val_loss: 0.4684 - val_acc: 0.7655\n",
      "Epoch 427/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4530 - acc: 0.7682 - val_loss: 0.4697 - val_acc: 0.7691\n",
      "Epoch 428/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4579 - acc: 0.7673 - val_loss: 0.4713 - val_acc: 0.7645\n",
      "Epoch 429/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4536 - acc: 0.7782 - val_loss: 0.4711 - val_acc: 0.7655\n",
      "Epoch 430/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4673 - acc: 0.7609 - val_loss: 0.4706 - val_acc: 0.7655\n",
      "Epoch 431/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4538 - acc: 0.7673 - val_loss: 0.4683 - val_acc: 0.7645\n",
      "Epoch 432/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4648 - acc: 0.7545 - val_loss: 0.4680 - val_acc: 0.7664\n",
      "Epoch 433/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4579 - acc: 0.7691 - val_loss: 0.4692 - val_acc: 0.7691\n",
      "Epoch 434/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4479 - acc: 0.7800 - val_loss: 0.4698 - val_acc: 0.7682\n",
      "Epoch 435/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4546 - acc: 0.7564 - val_loss: 0.4694 - val_acc: 0.7682\n",
      "Epoch 436/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4563 - acc: 0.7773 - val_loss: 0.4683 - val_acc: 0.7673\n",
      "Epoch 437/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4651 - acc: 0.7627 - val_loss: 0.4695 - val_acc: 0.7636\n",
      "Epoch 438/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4438 - acc: 0.7836 - val_loss: 0.4680 - val_acc: 0.7636\n",
      "Epoch 439/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4590 - acc: 0.7673 - val_loss: 0.4666 - val_acc: 0.7664\n",
      "Epoch 440/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4498 - acc: 0.7773 - val_loss: 0.4680 - val_acc: 0.7709\n",
      "Epoch 441/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4717 - acc: 0.7582 - val_loss: 0.4701 - val_acc: 0.7673\n",
      "Epoch 442/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4603 - acc: 0.7645 - val_loss: 0.4708 - val_acc: 0.7682\n",
      "Epoch 443/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4657 - acc: 0.7645 - val_loss: 0.4701 - val_acc: 0.7700\n",
      "Epoch 444/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4495 - acc: 0.7764 - val_loss: 0.4703 - val_acc: 0.7655\n",
      "Epoch 445/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4561 - acc: 0.7673 - val_loss: 0.4694 - val_acc: 0.7673\n",
      "Epoch 446/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4661 - acc: 0.7718 - val_loss: 0.4694 - val_acc: 0.7673\n",
      "Epoch 447/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4490 - acc: 0.7864 - val_loss: 0.4703 - val_acc: 0.7691\n",
      "Epoch 448/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4436 - acc: 0.7773 - val_loss: 0.4700 - val_acc: 0.7682\n",
      "Epoch 449/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4585 - acc: 0.7700 - val_loss: 0.4687 - val_acc: 0.7682\n",
      "Epoch 450/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4464 - acc: 0.7764 - val_loss: 0.4683 - val_acc: 0.7682\n",
      "Epoch 451/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4568 - acc: 0.7709 - val_loss: 0.4666 - val_acc: 0.7673\n",
      "Epoch 452/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4624 - acc: 0.7664 - val_loss: 0.4659 - val_acc: 0.7664\n",
      "Epoch 453/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4630 - acc: 0.7645 - val_loss: 0.4669 - val_acc: 0.7664\n",
      "Epoch 454/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4378 - acc: 0.7836 - val_loss: 0.4680 - val_acc: 0.7655\n",
      "Epoch 455/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4515 - acc: 0.7745 - val_loss: 0.4678 - val_acc: 0.7636\n",
      "Epoch 456/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4558 - acc: 0.7627 - val_loss: 0.4678 - val_acc: 0.7664\n",
      "Epoch 457/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4671 - acc: 0.7682 - val_loss: 0.4710 - val_acc: 0.7655\n",
      "Epoch 458/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4576 - acc: 0.7655 - val_loss: 0.4708 - val_acc: 0.7645\n",
      "Epoch 459/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4605 - acc: 0.7682 - val_loss: 0.4683 - val_acc: 0.7682\n",
      "Epoch 460/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4532 - acc: 0.7791 - val_loss: 0.4670 - val_acc: 0.7664\n",
      "Epoch 461/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4520 - acc: 0.7709 - val_loss: 0.4668 - val_acc: 0.7664\n",
      "Epoch 462/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4496 - acc: 0.7773 - val_loss: 0.4643 - val_acc: 0.7682\n",
      "Epoch 463/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4515 - acc: 0.7773 - val_loss: 0.4643 - val_acc: 0.7700\n",
      "Epoch 464/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4587 - acc: 0.7727 - val_loss: 0.4656 - val_acc: 0.7691\n",
      "Epoch 465/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4529 - acc: 0.7691 - val_loss: 0.4673 - val_acc: 0.7664\n",
      "Epoch 466/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4616 - acc: 0.7682 - val_loss: 0.4664 - val_acc: 0.7645\n",
      "Epoch 467/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4592 - acc: 0.7682 - val_loss: 0.4653 - val_acc: 0.7664\n",
      "Epoch 468/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4496 - acc: 0.7745 - val_loss: 0.4643 - val_acc: 0.7691\n",
      "Epoch 469/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4478 - acc: 0.7691 - val_loss: 0.4659 - val_acc: 0.7673\n",
      "Epoch 470/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4563 - acc: 0.7655 - val_loss: 0.4657 - val_acc: 0.7673\n",
      "Epoch 471/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4571 - acc: 0.7691 - val_loss: 0.4665 - val_acc: 0.7655\n",
      "Epoch 472/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4652 - acc: 0.7718 - val_loss: 0.4682 - val_acc: 0.7645\n",
      "Epoch 473/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4531 - acc: 0.7782 - val_loss: 0.4683 - val_acc: 0.7673\n",
      "Epoch 474/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4515 - acc: 0.7782 - val_loss: 0.4670 - val_acc: 0.7664\n",
      "Epoch 475/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4651 - acc: 0.7745 - val_loss: 0.4662 - val_acc: 0.7673\n",
      "Epoch 476/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4484 - acc: 0.7800 - val_loss: 0.4633 - val_acc: 0.7673\n",
      "Epoch 477/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4499 - acc: 0.7745 - val_loss: 0.4618 - val_acc: 0.7700\n",
      "Epoch 478/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4502 - acc: 0.7655 - val_loss: 0.4643 - val_acc: 0.7673\n",
      "Epoch 479/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4587 - acc: 0.7736 - val_loss: 0.4666 - val_acc: 0.7636\n",
      "Epoch 480/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4628 - acc: 0.7636 - val_loss: 0.4661 - val_acc: 0.7718\n",
      "Epoch 481/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4656 - acc: 0.7555 - val_loss: 0.4654 - val_acc: 0.7700\n",
      "Epoch 482/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4605 - acc: 0.7691 - val_loss: 0.4656 - val_acc: 0.7673\n",
      "Epoch 483/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4582 - acc: 0.7727 - val_loss: 0.4659 - val_acc: 0.7691\n",
      "Epoch 484/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4601 - acc: 0.7636 - val_loss: 0.4676 - val_acc: 0.7673\n",
      "Epoch 485/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4660 - acc: 0.7682 - val_loss: 0.4687 - val_acc: 0.7664\n",
      "Epoch 486/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4523 - acc: 0.7618 - val_loss: 0.4671 - val_acc: 0.7709\n",
      "Epoch 487/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4655 - acc: 0.7618 - val_loss: 0.4661 - val_acc: 0.7673\n",
      "Epoch 488/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4358 - acc: 0.7873 - val_loss: 0.4632 - val_acc: 0.7664\n",
      "Epoch 489/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4469 - acc: 0.7645 - val_loss: 0.4604 - val_acc: 0.7709\n",
      "Epoch 490/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4568 - acc: 0.7718 - val_loss: 0.4616 - val_acc: 0.7727\n",
      "Epoch 491/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4544 - acc: 0.7673 - val_loss: 0.4639 - val_acc: 0.7709\n",
      "Epoch 492/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4472 - acc: 0.7782 - val_loss: 0.4652 - val_acc: 0.7691\n",
      "Epoch 493/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4608 - acc: 0.7682 - val_loss: 0.4670 - val_acc: 0.7664\n",
      "Epoch 494/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4587 - acc: 0.7727 - val_loss: 0.4671 - val_acc: 0.7655\n",
      "Epoch 495/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4598 - acc: 0.7636 - val_loss: 0.4648 - val_acc: 0.7682\n",
      "Epoch 496/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4503 - acc: 0.7827 - val_loss: 0.4638 - val_acc: 0.7691\n",
      "Epoch 497/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4555 - acc: 0.7664 - val_loss: 0.4648 - val_acc: 0.7664\n",
      "Epoch 498/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4612 - acc: 0.7709 - val_loss: 0.4680 - val_acc: 0.7645\n",
      "Epoch 499/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4414 - acc: 0.7873 - val_loss: 0.4687 - val_acc: 0.7664\n",
      "Epoch 500/500\n",
      "1100/1100 [==============================] - 0s - loss: 0.4632 - acc: 0.7600 - val_loss: 0.4657 - val_acc: 0.7700\n",
      " 864/1100 [======================>.......] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46572553634643554, 0.7700000002167442]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train = encoder.predict(X_train)\n",
    "encoded_test = encoder.predict(X_test)\n",
    "\n",
    "encoder_input = Input(shape=(16,))\n",
    "encoded_features = Dense(8, activation='relu')(encoder_input)\n",
    "encoded_features = Dropout(0.25)(encoded_features)\n",
    "encoded_features = Dense(2, activation='softmax')(encoded_features)\n",
    "\n",
    "encoder_classifier = Model(input=encoder_input, output=encoded_features)\n",
    "\n",
    "encoder_classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "encoder_classifier.fit(encoded_train, y_train_onehot,\n",
    "                nb_epoch=500,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(encoded_test, y_test_onehot))\n",
    "\n",
    "encoder_classifier.evaluate(encoded_test, y_test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dt': {'accuracy': [0.93903548680618742,\n",
       "   0.93721565059144674,\n",
       "   0.91082802547770703,\n",
       "   0.93357597816196547,\n",
       "   0.92538671519563243,\n",
       "   0.93539581437670605,\n",
       "   0.91810737033666967,\n",
       "   0.93812556869881714,\n",
       "   0.92720655141037311,\n",
       "   0.92083712465878076,\n",
       "   0.91264786169244771,\n",
       "   0.93903548680618742,\n",
       "   0.93539581437670605,\n",
       "   0.92447679708826203,\n",
       "   0.9281164695177434,\n",
       "   0.92993630573248409,\n",
       "   0.90627843494085536,\n",
       "   0.91992720655141036,\n",
       "   0.93539581437670605,\n",
       "   0.92993630573248409],\n",
       "  'f': [0.94336283185840708,\n",
       "   0.93666369313113296,\n",
       "   0.91090909090909089,\n",
       "   0.93950177935943058,\n",
       "   0.92763731473408895,\n",
       "   0.92831541218637992,\n",
       "   0.9229374433363553,\n",
       "   0.94222222222222218,\n",
       "   0.93357271095152605,\n",
       "   0.91771019677996424,\n",
       "   0.9120580235720761,\n",
       "   0.94519317160826588,\n",
       "   0.93511111111111112,\n",
       "   0.91748003549245782,\n",
       "   0.92335115864527628,\n",
       "   0.93511111111111112,\n",
       "   0.91134751773049638,\n",
       "   0.92294065544729864,\n",
       "   0.94468832309043027,\n",
       "   0.92824704813805625],\n",
       "  'precision': [0.95195729537366547,\n",
       "   0.94936708860759489,\n",
       "   0.9424860853432282,\n",
       "   0.94821428571428568,\n",
       "   0.9263157894736842,\n",
       "   0.95963302752293578,\n",
       "   0.95140186915887848,\n",
       "   0.94623655913978499,\n",
       "   0.95238095238095233,\n",
       "   0.93682310469314078,\n",
       "   0.94589552238805974,\n",
       "   0.95612431444241319,\n",
       "   0.94774774774774773,\n",
       "   0.9422382671480144,\n",
       "   0.93309222423146476,\n",
       "   0.93917710196779969,\n",
       "   0.91771019677996424,\n",
       "   0.93202146690518783,\n",
       "   0.94551845342706498,\n",
       "   0.95009242144177453],\n",
       "  'recall': [0.94190140845070425,\n",
       "   0.92781690140845074,\n",
       "   0.89436619718309862,\n",
       "   0.926056338028169,\n",
       "   0.9348591549295775,\n",
       "   0.91725352112676062,\n",
       "   0.897887323943662,\n",
       "   0.93133802816901412,\n",
       "   0.91373239436619713,\n",
       "   0.903169014084507,\n",
       "   0.89084507042253525,\n",
       "   0.91725352112676062,\n",
       "   0.926056338028169,\n",
       "   0.9119718309859155,\n",
       "   0.91373239436619713,\n",
       "   0.93133802816901412,\n",
       "   0.91725352112676062,\n",
       "   0.91725352112676062,\n",
       "   0.94718309859154926,\n",
       "   0.90669014084507038],\n",
       "  'test_time': [0.0001506805419921875,\n",
       "   0.0001404285430908203,\n",
       "   0.00019431114196777344,\n",
       "   0.00014829635620117188,\n",
       "   0.0003082752227783203,\n",
       "   0.0001380443572998047,\n",
       "   0.00014162063598632812,\n",
       "   0.00014495849609375,\n",
       "   0.00014090538024902344,\n",
       "   0.00015878677368164062,\n",
       "   0.0001423358917236328,\n",
       "   0.00030541419982910156,\n",
       "   0.00014090538024902344,\n",
       "   0.00021386146545410156,\n",
       "   0.00013899803161621094,\n",
       "   0.00017380714416503906,\n",
       "   0.00014281272888183594,\n",
       "   0.00014638900756835938,\n",
       "   0.00015282630920410156,\n",
       "   0.00014781951904296875],\n",
       "  'train_time': [0.004834651947021484,\n",
       "   0.00448298454284668,\n",
       "   0.00464320182800293,\n",
       "   0.004480123519897461,\n",
       "   0.005496978759765625,\n",
       "   0.004324197769165039,\n",
       "   0.0044481754302978516,\n",
       "   0.004716157913208008,\n",
       "   0.004705190658569336,\n",
       "   0.004445791244506836,\n",
       "   0.004528999328613281,\n",
       "   0.005484819412231445,\n",
       "   0.0043337345123291016,\n",
       "   0.005034685134887695,\n",
       "   0.004335165023803711,\n",
       "   0.004722118377685547,\n",
       "   0.004695892333984375,\n",
       "   0.004392385482788086,\n",
       "   0.004735231399536133,\n",
       "   0.004692792892456055]},\n",
       " 'gbt': {'accuracy': [0.96633303002729753,\n",
       "   0.97270245677888989,\n",
       "   0.96178343949044587,\n",
       "   0.96633303002729753,\n",
       "   0.96997270245677891,\n",
       "   0.96360327570518656,\n",
       "   0.96178343949044587,\n",
       "   0.97452229299363058,\n",
       "   0.97452229299363058,\n",
       "   0.96269335759781616,\n",
       "   0.96360327570518656,\n",
       "   0.9717925386715196,\n",
       "   0.96178343949044587,\n",
       "   0.96633303002729753,\n",
       "   0.96997270245677891,\n",
       "   0.96269335759781616,\n",
       "   0.96269335759781616,\n",
       "   0.96724294813466793,\n",
       "   0.96724294813466793,\n",
       "   0.96724294813466793],\n",
       "  'f': [0.96687555953446724,\n",
       "   0.9732142857142857,\n",
       "   0.96202531645569622,\n",
       "   0.96681614349775791,\n",
       "   0.97035040431266839,\n",
       "   0.96376811594202894,\n",
       "   0.9621621621621621,\n",
       "   0.9748653500897666,\n",
       "   0.97500000000000009,\n",
       "   0.96322869955156964,\n",
       "   0.96409335727109513,\n",
       "   0.97239536954585937,\n",
       "   0.9621621621621621,\n",
       "   0.96657633242999086,\n",
       "   0.97056199821587863,\n",
       "   0.96342551293487955,\n",
       "   0.96316262353998194,\n",
       "   0.96791443850267389,\n",
       "   0.96774193548387089,\n",
       "   0.9676840215439857],\n",
       "  'precision': [0.98360655737704916,\n",
       "   0.9873188405797102,\n",
       "   0.98884758364312264,\n",
       "   0.98537477148080443,\n",
       "   0.99082568807339455,\n",
       "   0.9925373134328358,\n",
       "   0.98523985239852396,\n",
       "   0.99450549450549453,\n",
       "   0.98913043478260865,\n",
       "   0.98171846435100552,\n",
       "   0.98351648351648346,\n",
       "   0.98378378378378384,\n",
       "   0.98523985239852396,\n",
       "   0.99257884972170685,\n",
       "   0.98372513562386976,\n",
       "   0.97649186256781195,\n",
       "   0.98348623853211015,\n",
       "   0.98014440433213001,\n",
       "   0.98540145985401462,\n",
       "   0.98717948717948723],\n",
       "  'recall': [0.95070422535211263,\n",
       "   0.95950704225352113,\n",
       "   0.93661971830985913,\n",
       "   0.948943661971831,\n",
       "   0.95070422535211263,\n",
       "   0.93661971830985913,\n",
       "   0.9401408450704225,\n",
       "   0.95598591549295775,\n",
       "   0.96126760563380287,\n",
       "   0.94542253521126762,\n",
       "   0.94542253521126762,\n",
       "   0.96126760563380287,\n",
       "   0.9401408450704225,\n",
       "   0.94190140845070425,\n",
       "   0.95774647887323938,\n",
       "   0.95070422535211263,\n",
       "   0.94366197183098588,\n",
       "   0.95598591549295775,\n",
       "   0.95070422535211263,\n",
       "   0.948943661971831],\n",
       "  'test_time': [0.002094268798828125,\n",
       "   0.0021123886108398438,\n",
       "   0.0021791458129882812,\n",
       "   0.002034902572631836,\n",
       "   0.002134561538696289,\n",
       "   0.0020749568939208984,\n",
       "   0.0021741390228271484,\n",
       "   0.002125978469848633,\n",
       "   0.0021038055419921875,\n",
       "   0.0023543834686279297,\n",
       "   0.0021991729736328125,\n",
       "   0.002084493637084961,\n",
       "   0.002071857452392578,\n",
       "   0.0020678043365478516,\n",
       "   0.0022733211517333984,\n",
       "   0.002079010009765625,\n",
       "   0.0021262168884277344,\n",
       "   0.0020825862884521484,\n",
       "   0.002098560333251953,\n",
       "   0.0020475387573242188],\n",
       "  'train_time': [0.21478891372680664,\n",
       "   0.21840214729309082,\n",
       "   0.21852493286132812,\n",
       "   0.21299290657043457,\n",
       "   0.21638226509094238,\n",
       "   0.21589183807373047,\n",
       "   0.22074365615844727,\n",
       "   0.21730852127075195,\n",
       "   0.22079706192016602,\n",
       "   0.21326088905334473,\n",
       "   0.22453546524047852,\n",
       "   0.22528886795043945,\n",
       "   0.21913576126098633,\n",
       "   0.21923136711120605,\n",
       "   0.2192976474761963,\n",
       "   0.22132039070129395,\n",
       "   0.21299242973327637,\n",
       "   0.21335315704345703,\n",
       "   0.2124631404876709,\n",
       "   0.21280169486999512]},\n",
       " 'knn': {'accuracy': [0.75614194722474981,\n",
       "   0.70518653321201097,\n",
       "   0.75523202911737941,\n",
       "   0.73976342129208372,\n",
       "   0.7142857142857143,\n",
       "   0.74795268425841677,\n",
       "   0.73157415832575068,\n",
       "   0.72065514103730666,\n",
       "   0.72065514103730666,\n",
       "   0.70791628753412195,\n",
       "   0.7570518653321201,\n",
       "   0.74704276615104637,\n",
       "   0.71701546860782528,\n",
       "   0.73976342129208372,\n",
       "   0.75614194722474981,\n",
       "   0.72702456778889901,\n",
       "   0.73521383075523206,\n",
       "   0.76797088262056412,\n",
       "   0.72338489535941763,\n",
       "   0.73248407643312097],\n",
       "  'f': [0.7099567099567099,\n",
       "   0.62149532710280375,\n",
       "   0.70536692223439212,\n",
       "   0.6843267108167771,\n",
       "   0.62884160756501184,\n",
       "   0.70182992465016147,\n",
       "   0.67185761957730805,\n",
       "   0.64260768335273577,\n",
       "   0.6475315729047072,\n",
       "   0.6174016686531586,\n",
       "   0.7132116004296456,\n",
       "   0.70736842105263154,\n",
       "   0.63879210220673643,\n",
       "   0.677927927927928,\n",
       "   0.70678336980306355,\n",
       "   0.65753424657534243,\n",
       "   0.68403908794788271,\n",
       "   0.73629782833505697,\n",
       "   0.64651162790697669,\n",
       "   0.66284403669724778],\n",
       "  'precision': [0.9213483146067416,\n",
       "   0.92361111111111116,\n",
       "   0.93333333333333335,\n",
       "   0.91715976331360949,\n",
       "   0.95683453237410077,\n",
       "   0.90304709141274242,\n",
       "   0.91238670694864044,\n",
       "   0.94845360824742264,\n",
       "   0.93069306930693074,\n",
       "   0.955719557195572,\n",
       "   0.91460055096418735,\n",
       "   0.87958115183246077,\n",
       "   0.93856655290102387,\n",
       "   0.94062500000000004,\n",
       "   0.93352601156069359,\n",
       "   0.93506493506493504,\n",
       "   0.8923512747875354,\n",
       "   0.89223057644110271,\n",
       "   0.95205479452054798,\n",
       "   0.95065789473684215],\n",
       "  'recall': [0.57746478873239437,\n",
       "   0.46830985915492956,\n",
       "   0.56690140845070425,\n",
       "   0.54577464788732399,\n",
       "   0.46830985915492956,\n",
       "   0.573943661971831,\n",
       "   0.53169014084507038,\n",
       "   0.4859154929577465,\n",
       "   0.49647887323943662,\n",
       "   0.45598591549295775,\n",
       "   0.58450704225352113,\n",
       "   0.59154929577464788,\n",
       "   0.48415492957746481,\n",
       "   0.52992957746478875,\n",
       "   0.56866197183098588,\n",
       "   0.50704225352112675,\n",
       "   0.55457746478873238,\n",
       "   0.62676056338028174,\n",
       "   0.48943661971830987,\n",
       "   0.50880281690140849],\n",
       "  'test_time': [0.09620332717895508,\n",
       "   0.09423828125,\n",
       "   0.09456634521484375,\n",
       "   0.0945427417755127,\n",
       "   0.09423160552978516,\n",
       "   0.09400701522827148,\n",
       "   0.09434199333190918,\n",
       "   0.09428048133850098,\n",
       "   0.09488630294799805,\n",
       "   0.09442925453186035,\n",
       "   0.09486269950866699,\n",
       "   0.0952615737915039,\n",
       "   0.09574389457702637,\n",
       "   0.09645390510559082,\n",
       "   0.0952615737915039,\n",
       "   0.0941622257232666,\n",
       "   0.09579300880432129,\n",
       "   0.09434890747070312,\n",
       "   0.09428286552429199,\n",
       "   0.09514188766479492],\n",
       "  'train_time': [0.0017480850219726562,\n",
       "   0.001384735107421875,\n",
       "   0.0014104843139648438,\n",
       "   0.00138092041015625,\n",
       "   0.0013854503631591797,\n",
       "   0.0013990402221679688,\n",
       "   0.0013942718505859375,\n",
       "   0.0013628005981445312,\n",
       "   0.0014004707336425781,\n",
       "   0.0013914108276367188,\n",
       "   0.0013813972473144531,\n",
       "   0.0013895034790039062,\n",
       "   0.001394033432006836,\n",
       "   0.0017046928405761719,\n",
       "   0.0014026165008544922,\n",
       "   0.0013775825500488281,\n",
       "   0.0014352798461914062,\n",
       "   0.0013833045959472656,\n",
       "   0.0013890266418457031,\n",
       "   0.001371145248413086]},\n",
       " 'nn': {'accuracy': [],\n",
       "  'f': [],\n",
       "  'precision': [],\n",
       "  'recall': [],\n",
       "  'test_time': [],\n",
       "  'train_time': []},\n",
       " 'rf': {'accuracy': [0.92720655141037311,\n",
       "   0.94449499545040949,\n",
       "   0.94722474977252047,\n",
       "   0.93448589626933576,\n",
       "   0.92720655141037311,\n",
       "   0.94085532302092811,\n",
       "   0.93994540491355782,\n",
       "   0.93084622383985438,\n",
       "   0.93994540491355782,\n",
       "   0.92356687898089174,\n",
       "   0.93994540491355782,\n",
       "   0.93630573248407645,\n",
       "   0.92265696087352134,\n",
       "   0.94540491355777978,\n",
       "   0.9426751592356688,\n",
       "   0.94722474977252047,\n",
       "   0.9417652411282984,\n",
       "   0.92720655141037311,\n",
       "   0.93630573248407645,\n",
       "   0.94085532302092811],\n",
       "  'f': [0.94055013309671698,\n",
       "   0.93677649154051634,\n",
       "   0.94986807387862782,\n",
       "   0.93650793650793651,\n",
       "   0.9398023360287513,\n",
       "   0.93639575971731448,\n",
       "   0.93688888888888888,\n",
       "   0.9375,\n",
       "   0.93520140105078808,\n",
       "   0.92748433303491495,\n",
       "   0.93958521190261513,\n",
       "   0.94086496028243605,\n",
       "   0.92970123022847106,\n",
       "   0.9299820466786356,\n",
       "   0.9391304347826086,\n",
       "   0.93392857142857155,\n",
       "   0.93226381461675578,\n",
       "   0.92907801418439706,\n",
       "   0.94727435210008937,\n",
       "   0.93650793650793651],\n",
       "  'precision': [0.93262411347517726,\n",
       "   0.95017793594306055,\n",
       "   0.96336996336996339,\n",
       "   0.93829401088929221,\n",
       "   0.93816254416961131,\n",
       "   0.946524064171123,\n",
       "   0.94010889292196009,\n",
       "   0.96153846153846156,\n",
       "   0.94774774774774773,\n",
       "   0.93548387096774188,\n",
       "   0.94871794871794868,\n",
       "   0.97037037037037033,\n",
       "   0.94363636363636361,\n",
       "   0.93060498220640564,\n",
       "   0.9369527145359019,\n",
       "   0.9422382671480144,\n",
       "   0.96202531645569622,\n",
       "   0.94444444444444442,\n",
       "   0.96914700544464605,\n",
       "   0.95706618962432921],\n",
       "  'recall': [0.92781690140845074,\n",
       "   0.92253521126760563,\n",
       "   0.91901408450704225,\n",
       "   0.9348591549295775,\n",
       "   0.94190140845070425,\n",
       "   0.92429577464788737,\n",
       "   0.90845070422535212,\n",
       "   0.9348591549295775,\n",
       "   0.95598591549295775,\n",
       "   0.94190140845070425,\n",
       "   0.926056338028169,\n",
       "   0.93661971830985913,\n",
       "   0.91373239436619713,\n",
       "   0.903169014084507,\n",
       "   0.92253521126760563,\n",
       "   0.93838028169014087,\n",
       "   0.92077464788732399,\n",
       "   0.92957746478873238,\n",
       "   0.92957746478873238,\n",
       "   0.93133802816901412],\n",
       "  'test_time': [0.00145721435546875,\n",
       "   0.0014958381652832031,\n",
       "   0.0015819072723388672,\n",
       "   0.001466512680053711,\n",
       "   0.001523733139038086,\n",
       "   0.0014927387237548828,\n",
       "   0.0014832019805908203,\n",
       "   0.0015075206756591797,\n",
       "   0.0014760494232177734,\n",
       "   0.0018672943115234375,\n",
       "   0.0014977455139160156,\n",
       "   0.0014667510986328125,\n",
       "   0.0014874935150146484,\n",
       "   0.0014917850494384766,\n",
       "   0.0015184879302978516,\n",
       "   0.0015196800231933594,\n",
       "   0.0014836788177490234,\n",
       "   0.0015571117401123047,\n",
       "   0.0015201568603515625,\n",
       "   0.0015134811401367188],\n",
       "  'train_time': [0.013443231582641602,\n",
       "   0.01404714584350586,\n",
       "   0.017314434051513672,\n",
       "   0.013408660888671875,\n",
       "   0.013945341110229492,\n",
       "   0.015102863311767578,\n",
       "   0.01355433464050293,\n",
       "   0.015162229537963867,\n",
       "   0.01367950439453125,\n",
       "   0.014908075332641602,\n",
       "   0.014282703399658203,\n",
       "   0.013558149337768555,\n",
       "   0.01486515998840332,\n",
       "   0.014241695404052734,\n",
       "   0.01369929313659668,\n",
       "   0.016377687454223633,\n",
       "   0.013691425323486328,\n",
       "   0.015271425247192383,\n",
       "   0.01407480239868164,\n",
       "   0.014367818832397461]},\n",
       " 'svm': {'accuracy': [0.89626933575978163,\n",
       "   0.89717925386715192,\n",
       "   0.89535941765241134,\n",
       "   0.88080072793448594,\n",
       "   0.89444949954504094,\n",
       "   0.89444949954504094,\n",
       "   0.89080982711555956,\n",
       "   0.89535941765241134,\n",
       "   0.88626023657870789,\n",
       "   0.88808007279344858,\n",
       "   0.91264786169244771,\n",
       "   0.8989990900818926,\n",
       "   0.88808007279344858,\n",
       "   0.89535941765241134,\n",
       "   0.90718835304822565,\n",
       "   0.88989990900818927,\n",
       "   0.90536851683348496,\n",
       "   0.89626933575978163,\n",
       "   0.88989990900818927,\n",
       "   0.88626023657870789],\n",
       "  'f': [0.90000000000000002,\n",
       "   0.89955555555555566,\n",
       "   0.8966756513926325,\n",
       "   0.88355555555555554,\n",
       "   0.89492753623188404,\n",
       "   0.89568345323741017,\n",
       "   0.89208633093525169,\n",
       "   0.89592760180995468,\n",
       "   0.88967343336275373,\n",
       "   0.88928892889288924,\n",
       "   0.91304347826086962,\n",
       "   0.9004484304932735,\n",
       "   0.88928892889288924,\n",
       "   0.89592760180995468,\n",
       "   0.91021126760563387,\n",
       "   0.89339207048458158,\n",
       "   0.90545454545454551,\n",
       "   0.89929328621908122,\n",
       "   0.89206066012488849,\n",
       "   0.8884924174843889],\n",
       "  'precision': [0.89685314685314688,\n",
       "   0.90843806104129265,\n",
       "   0.91559633027522935,\n",
       "   0.8922800718132855,\n",
       "   0.92164179104477617,\n",
       "   0.9154411764705882,\n",
       "   0.91176470588235292,\n",
       "   0.92178770949720668,\n",
       "   0.89203539823008848,\n",
       "   0.90976058931860038,\n",
       "   0.94029850746268662,\n",
       "   0.91773308957952471,\n",
       "   0.90976058931860038,\n",
       "   0.92178770949720668,\n",
       "   0.91021126760563376,\n",
       "   0.89417989417989419,\n",
       "   0.93609022556390975,\n",
       "   0.90248226950354615,\n",
       "   0.9041591320072333,\n",
       "   0.90054249547920429],\n",
       "  'recall': [0.903169014084507,\n",
       "   0.89084507042253525,\n",
       "   0.87852112676056338,\n",
       "   0.875,\n",
       "   0.86971830985915488,\n",
       "   0.87676056338028174,\n",
       "   0.87323943661971826,\n",
       "   0.87147887323943662,\n",
       "   0.88732394366197187,\n",
       "   0.86971830985915488,\n",
       "   0.88732394366197187,\n",
       "   0.88380281690140849,\n",
       "   0.86971830985915488,\n",
       "   0.87147887323943662,\n",
       "   0.91021126760563376,\n",
       "   0.89260563380281688,\n",
       "   0.87676056338028174,\n",
       "   0.89612676056338025,\n",
       "   0.88028169014084512,\n",
       "   0.87676056338028174],\n",
       "  'test_time': [0.05973172187805176,\n",
       "   0.05976748466491699,\n",
       "   0.06045055389404297,\n",
       "   0.05936717987060547,\n",
       "   0.05973339080810547,\n",
       "   0.060923099517822266,\n",
       "   0.05879354476928711,\n",
       "   0.060598134994506836,\n",
       "   0.05913233757019043,\n",
       "   0.05823087692260742,\n",
       "   0.06274533271789551,\n",
       "   0.06081366539001465,\n",
       "   0.05915045738220215,\n",
       "   0.06042599678039551,\n",
       "   0.06208634376525879,\n",
       "   0.05778384208679199,\n",
       "   0.05927920341491699,\n",
       "   0.06320929527282715,\n",
       "   0.06180286407470703,\n",
       "   0.060194969177246094],\n",
       "  'train_time': [0.4855532646179199,\n",
       "   0.4812638759613037,\n",
       "   0.4838829040527344,\n",
       "   0.4771077632904053,\n",
       "   0.4832296371459961,\n",
       "   0.4817769527435303,\n",
       "   0.4793412685394287,\n",
       "   0.48246145248413086,\n",
       "   0.4761674404144287,\n",
       "   0.4741847515106201,\n",
       "   0.4947960376739502,\n",
       "   0.4862699508666992,\n",
       "   0.48606419563293457,\n",
       "   0.4847238063812256,\n",
       "   0.48743724822998047,\n",
       "   0.47740626335144043,\n",
       "   0.48018550872802734,\n",
       "   0.4860992431640625,\n",
       "   0.4871094226837158,\n",
       "   0.48624587059020996]}}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "results['knn'] = {}\n",
    "results['svm'] = {}\n",
    "results['dt'] = {}\n",
    "results['rf'] = {}\n",
    "results['nn'] = {}\n",
    "results['gbt'] = {}\n",
    "\n",
    "for result in results:\n",
    "    results[result]['accuracy'] = []\n",
    "    results[result]['precision'] = []\n",
    "    results[result]['recall'] = []\n",
    "    results[result]['f'] = []\n",
    "    results[result]['train_time'] = []\n",
    "    results[result]['test_time'] = []\n",
    "\n",
    "classifiers = {\n",
    "    'knn' : KNeighborsClassifier(n_neighbors=7),\n",
    "    'svm' : SVC(kernel='rbf', C=2, gamma=0.02, probability=True),\n",
    "    'dt'  : DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=1, max_leaf_nodes=(len(X_train) - 1)),\n",
    "    'rf' : RandomForestClassifier(min_samples_split=10, min_samples_leaf=1, max_leaf_nodes=(len(X_train) - 1)),\n",
    "    'gbt' : GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "for i in range(20):\n",
    "    idx = np.random.permutation(len(features_scaled))\n",
    "    features_scaled = features_scaled[idx]\n",
    "    labels = labels[idx]\n",
    "\n",
    "    X_train = features_scaled[:half]\n",
    "    X_test = features_scaled[half:]\n",
    "    y_train = labels[:half]\n",
    "    y_test = labels[half:]    \n",
    "    for clf_name in classifiers:\n",
    "        clf = classifiers[clf_name]\n",
    "        \n",
    "        train_start = time.time()\n",
    "        clf.fit(X_train, y_train)\n",
    "        train_end = time.time()\n",
    "        results[clf_name]['train_time'].append(train_end - train_start)\n",
    "        \n",
    "        test_start = time.time()\n",
    "        clf.predict(X_test)\n",
    "        test_end = time.time()\n",
    "        results[clf_name]['test_time'].append(test_end - test_start)     \n",
    "        \n",
    "        _, accuracy = cross_val_score(clf, features_scaled, labels, cv=2, scoring='accuracy')\n",
    "        _, precision = cross_val_score(clf, features_scaled, labels, cv=2, scoring='precision')\n",
    "        _, recall = cross_val_score(clf, features_scaled, labels, cv=2, scoring='recall')\n",
    "        _, f = cross_val_score(clf, features_scaled, labels, cv=2, scoring='f1')\n",
    "        results[clf_name]['accuracy'].append(accuracy)\n",
    "        results[clf_name]['precision'].append(precision)\n",
    "        results[clf_name]['recall'].append(recall)\n",
    "        results[clf_name]['f'].append(f)\n",
    "        \n",
    "    y_train_onehot = np_utils.to_categorical(y_train)\n",
    "    y_test_onehot = np_utils.to_categorical(y_test)\n",
    "    num_features = features.shape[1]\n",
    "    num_classes = len(np.unique(labels))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_features, input_dim=num_features, init='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, init='normal', activation='softmax'))    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    train_start = time.time()\n",
    "    model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), nb_epoch=100, batch_size=50, verbose=0)\n",
    "    train_end = time.time()\n",
    "    \n",
    "    loss, nn_acc = model.evaluate(X_test, y_test_onehot)\n",
    "    \n",
    "    test_start = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_end = time.time()\n",
    "    precision = average_precision_score(y_test_onehot, y_pred)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    recall = recall_score(y_test, y_pred)    \n",
    "    f1 = f1_score(y_test, y_pred)    \n",
    "    \n",
    "    results['nn']['accuracy'].append(nn_acc)\n",
    "    results['nn']['precision'].append(precision)\n",
    "    results['nn']['recall'].append(recall)\n",
    "    results['nn']['f'].append(f1)\n",
    "    results['nn']['train_time'].append(train_end - train_start)\n",
    "    results['nn']['test_time'].append(test_end - test_start)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn\n",
      "Mean\n",
      "accuracy      0.735123\n",
      "f             0.673126\n",
      "precision     0.926592\n",
      "recall        0.530810\n",
      "test_time     0.094852\n",
      "train_time    0.001424\n",
      "dtype: float64\n",
      "Standard deviation\n",
      "accuracy      0.018119\n",
      "f             0.034752\n",
      "precision     0.022454\n",
      "recall        0.048604\n",
      "test_time     0.000724\n",
      "train_time    0.000105\n",
      "dtype: float64\n",
      "dt\n",
      "Mean\n",
      "accuracy      0.927343\n",
      "f             0.928918\n",
      "precision     0.943722\n",
      "recall        0.918398\n",
      "test_time     0.000169\n",
      "train_time    0.004677\n",
      "dtype: float64\n",
      "Standard deviation\n",
      "accuracy      0.009899\n",
      "f             0.011352\n",
      "precision     0.010345\n",
      "recall        0.015234\n",
      "test_time     0.000051\n",
      "train_time    0.000335\n",
      "dtype: float64\n",
      "gbt\n",
      "Mean\n",
      "accuracy      0.966742\n",
      "f             0.967201\n",
      "precision     0.986033\n",
      "recall        0.949120\n",
      "test_time     0.002126\n",
      "train_time    0.217476\n",
      "dtype: float64\n",
      "Standard deviation\n",
      "accuracy      0.004273\n",
      "f             0.004298\n",
      "precision     0.004448\n",
      "recall        0.007830\n",
      "test_time     0.000078\n",
      "train_time    0.003916\n",
      "dtype: float64\n",
      "nn\n",
      "Mean\n",
      "accuracy      0.902636\n",
      "f             0.904254\n",
      "precision     0.958476\n",
      "recall        0.887089\n",
      "test_time     0.713635\n",
      "train_time    8.978221\n",
      "dtype: float64\n",
      "Standard deviation\n",
      "accuracy      0.009932\n",
      "f             0.009663\n",
      "precision     0.004793\n",
      "recall        0.013637\n",
      "test_time     0.042027\n",
      "train_time    0.372436\n",
      "dtype: float64\n",
      "svm\n",
      "Mean\n",
      "accuracy      0.894449\n",
      "f             0.896249\n",
      "precision     0.911142\n",
      "recall        0.882042\n",
      "test_time     0.060211\n",
      "train_time    0.483065\n",
      "dtype: float64\n",
      "Standard deviation\n",
      "accuracy      0.007630\n",
      "f             0.007329\n",
      "precision     0.013283\n",
      "recall        0.011678\n",
      "test_time     0.001431\n",
      "train_time    0.004827\n",
      "dtype: float64\n",
      "rf\n",
      "Mean\n",
      "accuracy      0.936806\n",
      "f             0.936765\n",
      "precision     0.947962\n",
      "recall        0.928169\n",
      "test_time     0.001520\n",
      "train_time    0.014450\n",
      "dtype: float64\n",
      "Standard deviation\n",
      "accuracy      0.007830\n",
      "f             0.005660\n",
      "precision     0.012116\n",
      "recall        0.012299\n",
      "test_time     0.000087\n",
      "train_time    0.001025\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for clf_name in results:\n",
    "    print(clf_name)\n",
    "    print(\"Mean\")\n",
    "    print(pd.DataFrame.from_dict(results[clf_name]).mean())\n",
    "    print(\"Standard deviation\")\n",
    "    print(pd.DataFrame.from_dict(results[clf_name]).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying autoencoder classification on data D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.336669803\n",
      "Epoch: 0002 cost= 1.252686620\n",
      "Epoch: 0003 cost= 1.194153786\n",
      "Epoch: 0004 cost= 1.153623581\n",
      "Epoch: 0005 cost= 1.114895344\n",
      "Epoch: 0006 cost= 1.075670719\n",
      "Epoch: 0007 cost= 1.042390943\n",
      "Epoch: 0008 cost= 1.020548820\n",
      "Epoch: 0009 cost= 1.011955380\n",
      "Epoch: 0010 cost= 1.009491205\n",
      "Epoch: 0011 cost= 1.008293152\n",
      "Epoch: 0012 cost= 1.007159948\n",
      "Epoch: 0013 cost= 1.005833626\n",
      "Epoch: 0014 cost= 1.004251361\n",
      "Epoch: 0015 cost= 1.002456307\n",
      "Epoch: 0016 cost= 1.000298738\n",
      "Epoch: 0017 cost= 0.997825623\n",
      "Epoch: 0018 cost= 0.995283306\n",
      "Epoch: 0019 cost= 0.992772877\n",
      "Epoch: 0020 cost= 0.990368605\n",
      "Epoch: 0021 cost= 0.988036036\n",
      "Epoch: 0022 cost= 0.985637069\n",
      "Epoch: 0023 cost= 0.983090162\n",
      "Epoch: 0024 cost= 0.980468512\n",
      "Epoch: 0025 cost= 0.977868617\n",
      "Epoch: 0026 cost= 0.975325406\n",
      "Epoch: 0027 cost= 0.972688615\n",
      "Epoch: 0028 cost= 0.970006168\n",
      "Epoch: 0029 cost= 0.967246771\n",
      "Epoch: 0030 cost= 0.964479506\n",
      "Epoch: 0031 cost= 0.961797357\n",
      "Epoch: 0032 cost= 0.959145308\n",
      "Epoch: 0033 cost= 0.956584752\n",
      "Epoch: 0034 cost= 0.954131424\n",
      "Epoch: 0035 cost= 0.951728404\n",
      "Epoch: 0036 cost= 0.949375927\n",
      "Epoch: 0037 cost= 0.947060347\n",
      "Epoch: 0038 cost= 0.944780469\n",
      "Epoch: 0039 cost= 0.942459345\n",
      "Epoch: 0040 cost= 0.940114617\n",
      "Epoch: 0041 cost= 0.937803864\n",
      "Epoch: 0042 cost= 0.935531616\n",
      "Epoch: 0043 cost= 0.933295429\n",
      "Epoch: 0044 cost= 0.931082487\n",
      "Epoch: 0045 cost= 0.928895772\n",
      "Epoch: 0046 cost= 0.926753402\n",
      "Epoch: 0047 cost= 0.924680531\n",
      "Epoch: 0048 cost= 0.922682524\n",
      "Epoch: 0049 cost= 0.920745969\n",
      "Epoch: 0050 cost= 0.918858230\n",
      "Epoch: 0051 cost= 0.917027056\n",
      "Epoch: 0052 cost= 0.915255785\n",
      "Epoch: 0053 cost= 0.913519740\n",
      "Epoch: 0054 cost= 0.911801159\n",
      "Epoch: 0055 cost= 0.910089731\n",
      "Epoch: 0056 cost= 0.908385277\n",
      "Epoch: 0057 cost= 0.906696141\n",
      "Epoch: 0058 cost= 0.905028045\n",
      "Epoch: 0059 cost= 0.903381348\n",
      "Epoch: 0060 cost= 0.901751399\n",
      "Epoch: 0061 cost= 0.900146723\n",
      "Epoch: 0062 cost= 0.898577094\n",
      "Epoch: 0063 cost= 0.897025645\n",
      "Epoch: 0064 cost= 0.895515859\n",
      "Epoch: 0065 cost= 0.894062638\n",
      "Epoch: 0066 cost= 0.892656982\n",
      "Epoch: 0067 cost= 0.891284168\n",
      "Epoch: 0068 cost= 0.889931738\n",
      "Epoch: 0069 cost= 0.888602316\n",
      "Epoch: 0070 cost= 0.887305737\n",
      "Epoch: 0071 cost= 0.886041224\n",
      "Epoch: 0072 cost= 0.884807765\n",
      "Epoch: 0073 cost= 0.883597910\n",
      "Epoch: 0074 cost= 0.882409036\n",
      "Epoch: 0075 cost= 0.881244063\n",
      "Epoch: 0076 cost= 0.880106747\n",
      "Epoch: 0077 cost= 0.879000902\n",
      "Epoch: 0078 cost= 0.877924144\n",
      "Epoch: 0079 cost= 0.876874804\n",
      "Epoch: 0080 cost= 0.875850499\n",
      "Epoch: 0081 cost= 0.874845803\n",
      "Epoch: 0082 cost= 0.873857915\n",
      "Epoch: 0083 cost= 0.872883856\n",
      "Epoch: 0084 cost= 0.871920824\n",
      "Epoch: 0085 cost= 0.870965719\n",
      "Epoch: 0086 cost= 0.870014966\n",
      "Epoch: 0087 cost= 0.869067967\n",
      "Epoch: 0088 cost= 0.868120909\n",
      "Epoch: 0089 cost= 0.867171168\n",
      "Epoch: 0090 cost= 0.866214991\n",
      "Epoch: 0091 cost= 0.865252018\n",
      "Epoch: 0092 cost= 0.864278495\n",
      "Epoch: 0093 cost= 0.863299608\n",
      "Epoch: 0094 cost= 0.862332821\n",
      "Epoch: 0095 cost= 0.861398280\n",
      "Epoch: 0096 cost= 0.860499620\n",
      "Epoch: 0097 cost= 0.859628379\n",
      "Epoch: 0098 cost= 0.858773232\n",
      "Epoch: 0099 cost= 0.857928455\n",
      "Epoch: 0100 cost= 0.857096374\n",
      "Optimization Finished!\n",
      "0.518182\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 32 # 1st layer num features\n",
    "n_hidden_2 = 16 # 2nd layer num features\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_features])\n",
    "y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_features, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, num_features])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_features])),\n",
    "}\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "encoded = tf.add(tf.matmul(encoder_op, tf.Variable(tf.random_normal([16, 2]))),\n",
    "                                       tf.Variable(tf.random_normal([2])))\n",
    "\n",
    "is_correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(encoded), 1), tf.argmax(y, 1))\n",
    "encoded_accuracy = tf.reduce_mean(tf.cast(is_correct_prediction, tf.float32))\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "\n",
    "    total_batches = int(len(X_train)/batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        start_time = time.time()\n",
    "        for i in range(total_batches):\n",
    "            batch_start = i * batch_size\n",
    "            batch_xs = X_train[batch_start:batch_start + batch_size]\n",
    "            batch_ys = y_train_onehot[batch_start:batch_start + batch_size]                 \n",
    "                 \n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs, y: batch_ys})\n",
    "                 \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"cost=\", \"{:.9f}\".format(c))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Applying encode and decode over test set\n",
    "    encode_decode = sess.run(\n",
    "        encoded_accuracy, feed_dict={X: X_test, y: y_test_onehot})\n",
    "    print(encode_decode)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
